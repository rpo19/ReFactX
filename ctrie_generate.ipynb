{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741e03e3-91ba-48c5-9f20-6ba1ee145dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1aae2a-1466-4d5b-aef4-3f12fb665cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, StoppingCriteria, StoppingCriteriaList, DynamicCache, OffloadedCache #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255d12e2-f799-4897-a15f-d1fdc285c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03167244-77c4-41b6-9698-7d9b412c1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27aeab6-d6d9-47b5-ad49-1357ce197eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_connection = psycopg.connect('postgres://postgres:secret@10.0.0.118:5432/postgres', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d58190-e8d0-4ee8-b16a-e0b59f9b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4432e09-49df-4af6-a17a-d2348ff2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootkey = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567e0bb8-1be5-4fef-9652-504d64d1c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2feac10f-ab0a-411e-a153-85954dac6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 408, 29, 662]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<end> .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "787271bd-c836-4d18-8e35-5d969e1b4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = {'ab':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae52940a-d605-4d7c-aef1-b8ef7b3526e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_triple = 662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0444330-37a1-4a58-97bc-ec68d82e0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.amazonaws', 'ov', '.amazonaws']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([29871, 869, 29871])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0364e3f4-c364-476c-8a35-1986335c8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rootkey > max(tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa01da77-361d-4ace-936e-55f04d335349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a991d2e-78df-48e9-a0b6-19e148cd20c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a825af7753c4c1c9f669fe09dc621a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype='bfloat16'\n",
    "            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #trust_remote_code=True,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             #attn_implementation=\"flash_attention_2\",\n",
    "                                             #attn_implementation=\"flash_attention\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6645b9-3be0-4b57-9fcc-138a93e4c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.device.type == device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c2b54b3-6c0d-45ee-abdd-dff9f66718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': '''You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "5) You provide a short concise answer.\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '''Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': '''\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "'''\n",
    "    }]\n",
    "\n",
    "prompt2 = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': '''You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
    "5) You provide a short concise answer.\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '''Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': '''Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Now I need the height of Mount Rainier.\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "'''\n",
    "    }]\n",
    "\n",
    "prompt3 = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': '''You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
    "5) You provide a short concise answer.\n",
    "\n",
    "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
    "In these cases you either:\n",
    "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
    "b) in case you don\\'t know the answer or you are not confident enough, you just answer \"I don't know\".\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '''Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': '''Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Now I need the height of Mount Rainier.\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "'''\n",
    "    }]\n",
    "\n",
    "prompt = prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ec60bee-3eba-4cc2-9c66-d5595d27500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\\n1) You receive an input question.\\n2) You reason on the path you need to follow to reach the answer starting from the information in the question.\\n3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\\n4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\\n5) You provide a short concise answer.\\n\\nSometimes it could happen that there are no relevant facts in the knowledge base.\\nIn these cases you either:\\na) provide your own answer clearly stating that there were no supporting facts to your answer;\\nb) in case you don\\'t know the answer or you are not confident enough, you just answer \"I don\\'t know\".\\n'}, {'role': 'user', 'content': 'Which mountain is taller between Mont Blanc and Mount Rainier?\\n'}, {'role': 'assistant', 'content': 'Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\\nFact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\\nNow I need the height of Mount Rainier.\\nFact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\\nLong answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\\n\\nAnswer: Mont Blanc.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f340e2-8353-4f43-98d6-7e6c82bd4f2a",
   "metadata": {},
   "source": [
    "## Find switch pattern\n",
    "may be tokenizer dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27832621-8b67-4a05-9fa6-251000ed2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 17873, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ċ', 'Fact', ':']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern = tokenizer('''\n",
    "Fact:''').input_ids\n",
    "print(switch_pattern)\n",
    "tokenizer.convert_ids_to_tokens(switch_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0a7d17f-72f6-4c4f-a27f-90a7edb82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_pattern = [17873, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d9fa421-7175-4aad-b9a2-a3c5b56d0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import PostgresTrieIndex, DictIndex, ConstrainedLogitsProcessor, ConstrainedStateList, ConstrainedState, GetAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d08c2aa0-5493-4e5d-937a-322c5574f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_token = tokenizer('''\n",
    "''').input_ids[-1]\n",
    "newline_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97d9c5e3-f4ad-4953-b297-5f8acd05c10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100265"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {'role':'user', 'content': '''Which city is the capital of the country where the Eiffel Tower is?\n",
    "'''}\n",
    "question = {'role':'user', 'content': '''Who was the quarterback of the team that won Super Bowl 50?\n",
    "'''}\n",
    "prompted_texts = [tokenizer.apply_chat_template(prompt + [question], tokenize=False, add_generation_prompt=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03db7b6e-5c68-46db-b3c1-019c3e5d7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
      "5) You provide a short concise answer.\n",
      "\n",
      "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
      "In these cases you either:\n",
      "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
      "b) in case you don't know the answer or you are not confident enough, you just answer \"I don't know\".\n",
      "<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|><|im_start|>assistant<|im_sep|>Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|><|im_start|>user<|im_sep|>Who was the quarterback of the team that won Super Bowl 50?\n",
      "<|im_end|><|im_start|>assistant<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "print(prompted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e95dd90-3572-4a64-9ffb-f270247b2bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba821415-8673-44ad-9143-7d29dd141bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ċ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.convert_ids_to_tokens(newline_token) # tokenizer.tokenize('\\n')[0]\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompted_texts, return_tensors='pt', padding=True, padding_side='right')\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7e38b57-f1ab-4890-93a8-cbc7b1e78a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100264,   9125, 100266,   2675,    527,    264,  11190,   3488,  36864,\n",
       "          18328,    430,  23963,   1202,   4320,    389,  13363,    505,    264,\n",
       "           6677,   2385]], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "602c685e-da3e-4a25-9dfd-25d13664b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 16533, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16533, 25]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode('''\n",
    "Answer:'''))\n",
    "answer_tokens = [16533, 25]\n",
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f759180-6a93-413f-a4bb-bbaa3487b894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[366, 60704, 29]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' <Paris>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2cb3c92f-e187-4a7a-bba7-4f5337b23d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ<'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "894ebe31-131c-42dc-9e20-5407f42d6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_parentheses_right = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb0169e1-411f-4b9b-9812-b7d80a5c3e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100265"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57a042e4-f2e5-4b96-8063-d30721ccd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "getanswer = GetAnswer(answer_tokens, [newline_token, tokenizer.eos_token_id], all)\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    getanswer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "045a725d-8d4d-45e1-bbcb-981df8f943b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16533, 25]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f3454a9-9f45-4f61-8b27-09979b7e842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 339])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e309ab38-4878-4551-89d7-aa466d13b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_index = PostgresTrieIndex(\n",
    "    rootkey=rootkey, postgresql_connection=postgresql_connection, switch_parameter=6,\n",
    "    table_name='ctriev3', end_of_triple=end_of_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "928bc065-deb5-4f17-b108-691b210eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_65448/2801023739.py\u001b[0m(22)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO debug ctrie.py:436\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m    out = model.generate(\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m        \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m        \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
      "5) You provide a short concise answer.\n",
      "\n",
      "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
      "In these cases you either:\n",
      "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
      "b) in case you don't know the answer or you are not confident enough, you just answer \"I don't know\".<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|><|im_start|>assistant<|im_sep|>Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "50?<|im_end|><|im_start|>assistant<|im_sep|>Reasoning: To answer this question, I need to identify the team that won Super Bowl 50 and then determine who the quarterback was for that team during the game.\n",
      "Fact: <Super Bowl 50> <winner> <Denver Broncos> .\n",
      "Now I need to find out who the quarterback was for the Denver Broncos during Super Bowl 50.\n",
      "Fact: "
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "states = ConstrainedStateList(\n",
    "    [ConstrainedState(switch_pattern, newline_token,\n",
    "                      DictIndex(end_of_triple=postgres_index.end_of_triple)) for _ in range(num_beams)])\n",
    "\n",
    "constrained_processor = ConstrainedLogitsProcessor(\n",
    "    index=postgres_index,\n",
    "    end_token=newline_token, states=states)#, tokenizer=tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    getanswer.set_prompt(inputs.input_ids[0])\n",
    "\n",
    "    pdb.set_trace() # TODO debug ctrie.py:436\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=600,\n",
    "        streamer = streamer,\n",
    "        #do_sample = True,\n",
    "        #top_k=3,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=1,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        kwargs = {'constrained_state': states}, # passing state\n",
    "    )\n",
    "\n",
    "print('Elapsed', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dfb7cc7c-ed8d-4b48-9e97-c12b63641329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  <Super Bowl 50> <winner> <Denver Broncos> . [366, 19841, 20904, 220, 1135, 29, 366, 53245, 29, 366, 96301, 42694, 29, 662, 198]\n",
      "1  <Super Bowl 50> <participating team> <Denver Broncos> . [366, 19841, 20904, 220, 1135, 29, 366, 4581, 8608, 1113, 2128, 29, 366, 96301, 42694, 29, 662, 198]\n",
      "2  <Super Bowl 50> <participating team> <Carolina Panthers> . [366, 19841, 20904, 220, 1135, 29, 366, 4581, 8608, 1113, 2128, 29, 366, 9028, 59004, 45267, 29, 662, 198]\n",
      "3  <Super Bowl 50> <location> <Levi's Stadium> . [366, 19841, 20904, 220, 1135, 29, 366, 2588, 29, 366, 2356, 10176, 596, 23462, 29, 662, 198]\n",
      "4  <Super Bowl 50> <attendance> <+71088> . [366, 19841, 20904, 220, 1135, 29, 366, 63725, 29, 86803, 19027, 2421, 29, 662, 198]\n",
      "5  <Super Bowl 50> <point in time> <2016-02-07T00:00:00Z> . [366, 19841, 20904, 220, 1135, 29, 366, 2837, 304, 892, 29, 366, 679, 21, 12, 2437, 12, 2589, 51, 410, 25, 410, 25, 410, 57, 29, 662, 198]\n",
      "6  <Super Bowl 50> <sport> <American football> . [366, 19841, 20904, 220, 1135, 29, 366, 62001, 29, 366, 29518, 9141, 29, 662, 198]\n",
      "7  <Super Bowl 50> <description> <2016 edition of the Super Bowl> . [366, 19841, 20904, 220, 1135, 29, 366, 4789, 29, 366, 679, 21, 14002, 315, 279, 7445, 20904, 29, 662, 198]\n",
      "8  <Super Bowl 50> <has part(s)> <Super Bowl 50 halftime show> . [366, 19841, 20904, 220, 1135, 29, 366, 4752, 961, 1161, 16401, 366, 19841, 20904, 220, 1135, 79559, 1501, 29, 662, 198]\n",
      "9  <Super Bowl 50> <instance of> <Super Bowl> . [366, 19841, 20904, 220, 1135, 29, 366, 4956, 315, 29, 366, 19841, 20904, 29, 662, 198]\n",
      "10  <Super Bowl 50> <followed by> <Super Bowl LI> . [366, 19841, 20904, 220, 1135, 29, 366, 19070, 291, 555, 29, 366, 19841, 20904, 7708, 29, 662, 198]\n",
      "11  <Super Bowl 50> <follows> <Super Bowl XLIX> . [366, 19841, 20904, 220, 1135, 29, 366, 19070, 82, 29, 366, 19841, 20904, 30981, 5511, 29, 662, 198]\n",
      "12  <Super Bowl 50> <short description> <2016 National Football League championship game> . [366, 19841, 20904, 220, 1135, 29, 366, 8846, 4096, 29, 366, 679, 21, 5165, 21424, 9130, 22279, 1847, 29, 662, 198]\n",
      "13  <Super Bowl 50> <country> <United States> . [366, 19841, 20904, 220, 1135, 29, 366, 11389, 29, 366, 23175, 4273, 29, 662, 198]\n",
      "14  <Super Bowl 50 halftime show> <part of> <Super Bowl 50> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 4581, 315, 29, 366, 19841, 20904, 220, 1135, 29, 662, 198]\n",
      "15  <Super Bowl 50 halftime show> <short description> <2016 show headlined by Coldplay> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 8846, 4096, 29, 366, 679, 21, 1501, 2010, 15472, 555, 24062, 1387, 29, 662, 198]\n",
      "16  <Super Bowl 50 halftime show> <sport> <American football> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 62001, 29, 366, 29518, 9141, 29, 662, 198]\n",
      "17  <Super Bowl 50 halftime show> <instance of> <Super Bowl halftime show (halftime show during the Super Bowl)> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 4956, 315, 29, 366, 19841, 20904, 79559, 1501, 320, 12130, 18267, 1501, 2391, 279, 7445, 20904, 16401, 662, 198]\n",
      "18  <Super Bowl 50 halftime show> <country> <United States> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 11389, 29, 366, 23175, 4273, 29, 662, 198]\n",
      "19  <Super Bowl 50 halftime show> <description> <2016 show headlined by Coldplay> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 4789, 29, 366, 679, 21, 1501, 2010, 15472, 555, 24062, 1387, 29, 662, 198]\n",
      "20  <Super Bowl 50 halftime show> <point in time> <2016-02-07T00:00:00Z> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 2837, 304, 892, 29, 366, 679, 21, 12, 2437, 12, 2589, 51, 410, 25, 410, 25, 410, 57, 29, 662, 198]\n",
      "21  <Super Bowl 50 halftime show> <participant> <Beyoncé> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 59613, 29, 366, 33, 1216, 95710, 29, 662, 198]\n",
      "22  <Super Bowl 50 halftime show> <participant> <Bruno Mars> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 59613, 29, 366, 95330, 78, 21725, 29, 662, 198]\n",
      "23  <Super Bowl 50 halftime show> <participant> <Coldplay> . [366, 19841, 20904, 220, 1135, 79559, 1501, 29, 366, 59613, 29, 366, 77518, 1387, 29, 662, 198]\n"
     ]
    }
   ],
   "source": [
    "for i, triple in enumerate(states[0].generated_triples):\n",
    "    print(i, tokenizer.decode(triple)[:-1], triple, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af99308-f689-45be-a78e-f3615f114229",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([150000, 366, 60704, 29, 366, 66163, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d767433a-8f7d-4be4-80cf-d32d858ad4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġ<',\n",
       " 'Paris',\n",
       " '>',\n",
       " 'Ġ<',\n",
       " 'capital',\n",
       " 'Ġof',\n",
       " '>',\n",
       " 'Ġ<',\n",
       " 'France',\n",
       " '>',\n",
       " 'Ġ.',\n",
       " 'Ċ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([366, 60704, 29, 366, 66163, 315, 29, 366, 50100, 29, 662, 198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcd4f798-6cee-4836-b7f7-5d3b8a877c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states[0].generated_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e4bb8a2-b5a5-4409-b32b-dc9e2001a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states[0].cache_index.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "040e7587-2da7-4869-b4a9-c2dc7fdfec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states[0].cache_index.count_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58b5e54f-5d9a-4a2d-badb-7fbafd59ae33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[17, [150000, 366, 36, 3168, 301, 22703, 29, 366, {\\n    8846: [1, [4096, 29, 366, 96924, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]],\\n    4789: [2, [29, 366, {\\n        78678: [1, [7559, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]],\\n        34717: [1, [287, 555, 8563, 7462, 64, 359, 352, 304, 279, 80735, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]]}, 7559, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]], # deforme!\\n    34713: [1, [29, 366, 13020, 58618, 29, 662]],\\n    4956: [1, [315, 29, 366, 34717, 287, 320, 30318, 29409, 11, 7479, 10255, 2740, 9960, 449, 6308, 16401, 662]],\\n    33498: [1, [29, 366, 35632, 7462, 64, 359, 352, 29, 662]],\\n    258: [1, [1010, 29, 366, 5926, 21, 12, 1721, 12, 1721, 51, 410, 25, 410, 25, 410, 57, 29, 662]],\\n    28010: [2, [505, 3769, 29, 366, {\\n        20538: [1, [320, 34717, 287, 7479, 1903, 315, 9193, 8987, 55618, 14733, 2695, 16651, 13354, 16401, 662]],\\n        78009: [1, [6308, 29, 662]]}]],\\n    13727: [1, [29, 366, 39, 404, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]],\\n    15237: [1, [31095, 29, 366, 36, 3168, 301, 22703, 29, 662]],\\n    3902: [1, [3917, 29, 366, 36, 3168, 301, 22703, 29, 662]],\\n    2627: [1, [29, 86803, 6330, 13, 21, 29, 662]],\\n    3175: [1, [29, 86803, 4364, 29, 662]],\\n    2588: [1, [29, 366, 39, 404, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]],\\n    15859: [1, [2704, 29, 366, 12965, 8106, 29, 662]],\\n    11389: [1, [29, 366, 23175, 4273, 29, 662]],\\n    4581: [1, [315, 279, 4101, 29, 366, 36, 3168, 301, 22703, 320, 35, 8458, 359, 352, 4101, 16401, 662]]}]]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO subtree deforme e con countleaves sbagliato. è dopo il merge. quindi magari il priblema è il merge.\n",
    "'''[17, [150000, 366, 36, 3168, 301, 22703, 29, 366, {\n",
    "    8846: [1, [4096, 29, 366, 96924, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]],\n",
    "    4789: [2, [29, 366, {\n",
    "        78678: [1, [7559, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]],\n",
    "        34717: [1, [287, 555, 8563, 7462, 64, 359, 352, 304, 279, 80735, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]]}, 7559, 389, 279, 56690, 409, 21725, 304, 12366, 11, 9822, 29, 662]], # deforme!\n",
    "    34713: [1, [29, 366, 13020, 58618, 29, 662]],\n",
    "    4956: [1, [315, 29, 366, 34717, 287, 320, 30318, 29409, 11, 7479, 10255, 2740, 9960, 449, 6308, 16401, 662]],\n",
    "    33498: [1, [29, 366, 35632, 7462, 64, 359, 352, 29, 662]],\n",
    "    258: [1, [1010, 29, 366, 5926, 21, 12, 1721, 12, 1721, 51, 410, 25, 410, 25, 410, 57, 29, 662]],\n",
    "    28010: [2, [505, 3769, 29, 366, {\n",
    "        20538: [1, [320, 34717, 287, 7479, 1903, 315, 9193, 8987, 55618, 14733, 2695, 16651, 13354, 16401, 662]],\n",
    "        78009: [1, [6308, 29, 662]]}]],\n",
    "    13727: [1, [29, 366, 39, 404, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]],\n",
    "    15237: [1, [31095, 29, 366, 36, 3168, 301, 22703, 29, 662]],\n",
    "    3902: [1, [3917, 29, 366, 36, 3168, 301, 22703, 29, 662]],\n",
    "    2627: [1, [29, 86803, 6330, 13, 21, 29, 662]],\n",
    "    3175: [1, [29, 86803, 4364, 29, 662]],\n",
    "    2588: [1, [29, 366, 39, 404, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662]],\n",
    "    15859: [1, [2704, 29, 366, 12965, 8106, 29, 662]],\n",
    "    11389: [1, [29, 366, 23175, 4273, 29, 662]],\n",
    "    4581: [1, [315, 279, 4101, 29, 366, 36, 3168, 301, 22703, 320, 35, 8458, 359, 352, 4101, 16401, 662]]}]]'''\n",
    "# merge conta una leaf in meno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a65ea021-2c9a-40be-a23a-0bfb51c1444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19035cc7-5680-434b-aa2f-dc9e1a8b6124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states[0].generated_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76810e2e-d256-4f6e-990d-7093d3d0426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found 2 <Eiffel Tower> <location> <Philadelphia Museum of Art> .\n",
    "<Eiffel Tower> <location> <Philadelphia Museum of Art> .\n",
    "# but the index has 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca21f58-2cbd-4f5e-a5be-fe62a99a595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,[1,{3:4}]] == [1,[1,{3:4}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be944533-4eae-42f6-ab62-79ee05ee294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <Eiffel Tower> <location>'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([366, 36, 3168, 301, 22703, 29, 366, 2588, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a7c4f83-ec65-4407-bbd1-24696d76bacb",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyIndexException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyIndexException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#postgres_index.cache = DictIndex(end_of_triple=postgres_index.end_of_triple)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m postgres_index\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpostgres_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m150000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m366\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m36\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3168\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m301\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22703\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m366\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2588\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/notebooks/secondment/ctrie.py:96\u001b[0m, in \u001b[0;36mDictIndex.next_tokens\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     93\u001b[0m level_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyIndexException()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# visit the tree following sequence\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cursor \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;129;01mand\u001b[39;00m level_cursor \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(level[\u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mEmptyIndexException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#postgres_index.cache = DictIndex(end_of_triple=postgres_index.end_of_triple)\n",
    "postgres_index.cache.reset()\n",
    "postgres_index.cache.next_tokens([150000, 366, 36, 3168, 301, 22703, 29, 366, 2588, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dabd4686-366e-4330-b09a-1f6524b38f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{29: 17, 315: 1}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postgres_index.next_tokens([366, 36, 3168, 301, 22703, 29, 366, 2588])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "abd18e6f-e514-4aeb-aeb4-cec8a2f5c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(postgres_index.next_tokens([366, 36, 3168, 301, 22703, 29, 366, 2588, 315]).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9baa55c7-269d-450a-829f-f287e19d55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_50505/2324913377.py\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0mpostgres_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0mpostgres_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m366\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3168\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m301\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22703\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m366\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2588\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m366\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "5   breakpoint   keep yes   at /workspace/notebooks/secondment/ctrie.py:278\n",
      "\tbreakpoint already hit 1 time\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  clea 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: invalid syntax\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b /workspace/notebooks/secondment/ctrie.py:279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 6 at /workspace/notebooks/secondment/ctrie.py:279\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/workspace/notebooks/secondment/ctrie.py\u001b[0m(278)\u001b[0;36m_next_tokens_from_postgresql\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    276 \u001b[0;31m                        \u001b[0mcurrent_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    277 \u001b[0;31m                        \u001b[0mmerge_numleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_numleaves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m5\u001b[0;32m-> 278 \u001b[0;31m                        \u001b[0;32mif\u001b[0m \u001b[0mmerge_numleaves\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnumleaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m6\u001b[0;32m   279 \u001b[0;31m                            \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    280 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p merge_numleaves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p numleaves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "5   breakpoint   keep yes   at /workspace/notebooks/secondment/ctrie.py:278\n",
      "\tbreakpoint already hit 2 times\n",
      "6   breakpoint   keep yes   at /workspace/notebooks/secondment/ctrie.py:279\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  clear 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted breakpoint 5 at /workspace/notebooks/secondment/ctrie.py:278\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/workspace/notebooks/secondment/ctrie.py\u001b[0m(279)\u001b[0;36m_next_tokens_from_postgresql\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    277 \u001b[0;31m                        \u001b[0mmerge_numleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_numleaves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    278 \u001b[0;31m                        \u001b[0;32mif\u001b[0m \u001b[0mmerge_numleaves\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnumleaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m6\u001b[0;32m-> 279 \u001b[0;31m                            \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    280 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    281 \u001b[0;31m            \u001b[0msequence_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalnumleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p merge_numleaves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p numleaves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "pdb.set_trace()\n",
    "postgres_index.cache.reset()\n",
    "postgres_index.next_tokens([366, 36, 3168, 301, 22703, 29, 366, 2588, 29, 366])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0c67c2f-a27f-41de-a3b4-1422ded7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states[0].generated_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "35083703-ddc8-4c41-96c0-84df3a4a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{198: 2}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].cache_index.next_tokens([366,\n",
    " 36,\n",
    " 3168,\n",
    " 301,\n",
    " 22703,\n",
    " 29,\n",
    " 366,\n",
    " 2588,\n",
    " 29,\n",
    " 366,\n",
    " 39,\n",
    " 404,\n",
    " 939,\n",
    " 51984,\n",
    " 16730,\n",
    " 323,\n",
    " 83797,\n",
    " 554,\n",
    " 19558,\n",
    " 29,\n",
    " 662])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90af0d22-01c4-46b0-8214-124062a93818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " [366,\n",
       "  36,\n",
       "  3168,\n",
       "  301,\n",
       "  22703,\n",
       "  29,\n",
       "  366,\n",
       "  {2588: [2,\n",
       "    [29,\n",
       "     366,\n",
       "     {1163: [1, [1141, 409, 21725, 29, 662, 198]],\n",
       "      39: [1,\n",
       "       [404, 939, 51984, 16730, 323, 83797, 554, 19558, 29, 662, 198]]}]],\n",
       "   40563: [3,\n",
       "    [{304: [1,\n",
       "       [279,\n",
       "        23541,\n",
       "        52482,\n",
       "        5502,\n",
       "        29,\n",
       "        366,\n",
       "        22,\n",
       "        339,\n",
       "        2961,\n",
       "        2159,\n",
       "        50753,\n",
       "        315,\n",
       "        12366,\n",
       "        29,\n",
       "        662,\n",
       "        198]],\n",
       "      389: [2,\n",
       "       [8761,\n",
       "        29,\n",
       "        366,\n",
       "        {1163: [1, [1141, 409, 21725, 29, 662, 198]],\n",
       "         5389: [1,\n",
       "          [361,\n",
       "           96664,\n",
       "           1286,\n",
       "           7424,\n",
       "           35206,\n",
       "           320,\n",
       "           5389,\n",
       "           361,\n",
       "           304,\n",
       "           12366,\n",
       "           11,\n",
       "           9822,\n",
       "           16401,\n",
       "           662,\n",
       "           198]]}]]}]]}]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].cache_index.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "297a4a90-eb55-4b26-b541-7b8fe7247a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[366,\n",
       " 36,\n",
       " 3168,\n",
       " 301,\n",
       " 22703,\n",
       " 29,\n",
       " 366,\n",
       " 2588,\n",
       " 29,\n",
       " 366,\n",
       " 39,\n",
       " 404,\n",
       " 939,\n",
       " 51984,\n",
       " 16730,\n",
       " 323,\n",
       " 83797,\n",
       " 554,\n",
       " 19558,\n",
       " 29,\n",
       " 662,\n",
       " 198]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].generated_triples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb479f2a-31cf-40da-98a8-9c2348c791bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <Eiffel Tower> <location> <Hirshhorn Museum and Sculpture Garden>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([366,\n",
    " 36,\n",
    " 3168,\n",
    " 301,\n",
    " 22703,\n",
    " 29,\n",
    " 366,\n",
    " 2588,\n",
    " 29,\n",
    " 366,\n",
    " 39,\n",
    " 404,\n",
    " 939,\n",
    " 51984,\n",
    " 16730,\n",
    " 323,\n",
    " 83797,\n",
    " 554,\n",
    " 19558,\n",
    " 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24ab4895-f7f7-4882-84ef-89c9f96ea3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: states[0].generated_triples[-1] == x, states[0].generated_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f10172c2-2160-4ac1-a44e-43ee329e3bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55279,\n",
       " 219839,\n",
       " 102404,\n",
       " 269241,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169,\n",
       " 205169]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(sum,states[0].generated_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852abd27-fd30-4b40-9ca8-0a3d7c849730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO problem: in case of duplicates my approach does not work. either remove duplicates (sort wikidump and remove) or\n",
    "# WORKAROUND: generate duplicated up to duplication times.\n",
    "# TODO è stata generata la stessa triple -> non va bene\n",
    "# TODO controllare se c'è eiffel tower.\n",
    "# forse il nuovo sistema complica tutto e basta (anche se salva spazio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89aa77af-f66d-4428-806b-2cb965310062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ tensor(5151717, device='cuda:0') 400\n",
      "Reasoning: To answer this question, I need to identify the team that won Super Bowl 50 and then determine who the quarterback was for that team during the game.\n",
      "Fact: <Super Bowl 50> <winner> <Denver Broncos> .\n",
      "Now I need to find out who the quarterback was for the Denver Broncos during Super Bowl 50.\n",
      "Fact: <Super Bowl 50> <participating team> <Denver Broncos> .\n",
      "Fact: <Denver Broncos> <head coach> <Sean Payton> .\n",
      "Fact: <Denver Broncos> <headquarters location> <Denver> .\n",
      "Fact: <Denver Broncos> <home venue> <Empower Field at Mile High> .\n",
      "Fact: <Denver Broncos> <league or competition> <National Football League> .\n",
      "Fact: <Denver Broncos> <sport> <American football> .\n",
      "Fact: <Denver Broncos> <part of> <AFC West> .\n",
      "Fact: <Denver Broncos> <mascot> <Thunder (mascot)> .\n",
      "Fact: <Denver Broncos> <history of topic> <history of the Denver Broncos (aspect of history)> .\n",
      "Fact: <Denver Broncos> <inception> <1960-01-01T00:00:00Z> .\n",
      "Fact: <Denver Broncos> <short description> <National Football League franchise in Denver, Colorado> .\n",
      "Fact: <Denver Broncos> <topic's main template> <Template:Denver Broncos (Wikimedia template)> .\n",
      "Fact: <Denver Broncos> <country> <United States> .\n",
      "Fact: <Denver Broncos> <description> <National Football League franchise in Denver, Colorado> .\n",
      "Fact: <Denver Broncos> <owned by> <Pat Bowlen> .\n",
      "Fact: <Denver Broncos> <social media followers> <+140000> .\n",
      "Fact: <Denver Broncos> <social media followers> <+128786> .\n",
      "Fact:\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print('-'*30, sum(out[i][len(inputs.input_ids[0]):]), len(out[i][len(inputs.input_ids[0]):]))\n",
    "    print(tokenizer.decode(out[i][len(inputs.input_ids[0]):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "072f86f6-d4aa-465c-8ad9-82f467175679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
      "5) You provide a short concise answer.<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|><|im_start|>assistant<|im_sep|>Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.<|im_end|><|im_start|>user<|im_sep|>Who was the quarterback of the team that won Super Bowl 50?<|im_end|><|im_start|>assistant<|im_sep|>Reasoning: To answer this question, I need to identify the team that won Super Bowl 50 and then determine who the quarterback was for that team during the game.\n",
      "Fact: <Super Bowl 50> <winner> <Denver Broncos> .\n",
      "Now I need to find out who the quarterback was for the Denver Broncos during Super Bowl 50.\n",
      "Fact: <Super Bowl 50> <participating team> <Denver Broncos> .\n",
      "Fact: <Denver Broncos> <head coach> <Sean Payton> .\n",
      "Fact: <Denver Broncos> <headquarters location> <Denver> .\n",
      "Fact: <Denver Broncos> <home venue> <Empower Field at Mile High> .\n",
      "Fact: <Denver Broncos> <league or competition> <National Football League> .\n",
      "Fact: <Denver Broncos> <sport> <American football> .\n",
      "Fact: <Denver Broncos> <part of> <AFC West> .\n",
      "Fact: <Denver Broncos> <mascot> <Thunder (mascot)> .\n",
      "Fact: <Denver Broncos> <history of topic> <history of the Denver Broncos (aspect of history)> .\n",
      "Fact: <Denver Broncos> <inception> <1960-01-01T00:00:00Z> .\n",
      "Fact: <Denver Broncos> <short description> <National Football League franchise in Denver, Colorado> .\n",
      "Fact: <Denver Broncos> <topic's main template> <Template:Denver Broncos (Wikimedia template)> .\n",
      "Fact: <Denver Broncos> <country> <United States> .\n",
      "Fact: <Denver Broncos> <description> <National Football League franchise in Denver, Colorado> .\n",
      "Fact: <Denver Broncos> <owned by> <Pat Bowlen> .\n",
      "Fact: <Denver Broncos> <social media followers> <+140000> .\n",
      "Fact: <Denver Broncos> <social media followers> <+128786> .\n",
      "Fact:\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print(tokenizer.decode(out[i]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14f43663-500f-4f6a-b47c-8cbe8eaaba6c",
   "metadata": {},
   "source": [
    "<|im_start|>system<|im_sep|>\n",
    "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "5) You provide a short concise answer.\n",
    "<|im_end|><|im_start|>user<|im_sep|>\n",
    "\n",
    "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "\n",
    "Answer: Mont Blanc.<|im_end|><|im_start|>user<|im_sep|>\n",
    "\n",
    "Which city is the capital of the country where the Eiffel Tower is?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact:<Eiffel Tower> <location> <Champ de Mars> .\n",
    "Fact:<Champ de Mars> <city> <Paris> .\n",
    "Fact:<Paris> <capital of> <France> .\n",
    "\n",
    "Answer: Paris.<|im_end|>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a99f66a1-d5e5-4865-9757-e4ec309bb0a0",
   "metadata": {},
   "source": [
    "Who was the quarterback of the team that won Super Bowl 50?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact:<The Denver Post> <headquarters location> <Denver> .\n",
    "Fact:<Denver Broncos> <home city> <Denver> .\n",
    "Fact:<Super Bowl 50> <winning team> <Denver Broncos> .\n",
    "Fact:<Super Bowl 50> <date> <February 7, 2016> .\n",
    "Fact:<Peyton Manning> <position> <quarterback> .\n",
    "Fact:<Peyton Manning> <team during Super Bowl 50> <Denver Broncos> .\n",
    "\n",
    "Answer: Peyton Manning.<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9631cc0d-16d8-4d61-abeb-877401641bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, '')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop, ans = getanswer.get_answer(out[0])\n",
    "# TODO: is getanswer still needed?\n",
    "stop, tokenizer.decode(list(ans)) # TODO trim and remove special tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
