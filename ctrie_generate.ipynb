{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3beccbb-aa32-4dae-9ed5-6193fba6d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d12e2-f799-4897-a15f-d1fdc285c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03167244-77c4-41b6-9698-7d9b412c1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27aeab6-d6d9-47b5-ad49-1357ce197eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_connection = psycopg.connect('postgres://postgres:secret@10.0.0.118:5432/postgres', autocommit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d58190-e8d0-4ee8-b16a-e0b59f9b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4432e09-49df-4af6-a17a-d2348ff2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootkey = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0364e3f4-c364-476c-8a35-1986335c8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rootkey > max(tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa01da77-361d-4ace-936e-55f04d335349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a991d2e-78df-48e9-a0b6-19e148cd20c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             # attn_implementation=\"flash_attention_2\"\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2c2b54b3-6c0d-45ee-abdd-dff9f66718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''You are a question-answering system that reasons using structured data in the form of triples.\n",
    "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
    "Follow this format:\n",
    "\n",
    "Question: The question to be answered.\n",
    "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
    "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
    "Answer: the concise answer.\n",
    "\n",
    "Example:\n",
    "Question: Is Mont Blanc taller than Mount Rainier?\n",
    "Triples for the reasoning process:\n",
    "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
    "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
    "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
    "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
    "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
    "\n",
    "\n",
    "Now, answer the following question:\n",
    "Question: {}\n",
    "Triples for the reasoning process:\n",
    "triple:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ec60bee-3eba-4cc2-9c66-d5595d27500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a question-answering system that reasons using structured data in the form of triples.\n",
      "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Triples for the reasoning process:\n",
      "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: {}\n",
      "Triples for the reasoning process:\n",
      "triple:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f340e2-8353-4f43-98d6-7e6c82bd4f2a",
   "metadata": {},
   "source": [
    "## Find switch pattern\n",
    "may be tokenizer dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27832621-8b67-4a05-9fa6-251000ed2b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tri', 'ple', ':']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern = tokenizer('''\n",
    "triple:''').input_ids[2:]\n",
    "tokenizer.convert_ids_to_tokens(switch_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c0a7d17f-72f6-4c4f-a27f-90a7edb82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_pattern = [3626, 552, 29901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6d9fa421-7175-4aad-b9a2-a3c5b56d0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import ModDisjunctiveTrie, CtrieLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e309ab38-4878-4551-89d7-aa466d13b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "myctrie = ModDisjunctiveTrie(postgres_connection, rootkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d08c2aa0-5493-4e5d-937a-322c5574f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_token = tokenizer('''\n",
    "''').input_ids[-1]\n",
    "newline_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "97d9c5e3-f4ad-4953-b297-5f8acd05c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Who is the father of Queen Elizabeth the 2nd?'''\n",
    "prompted_text = prompt.format(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompted_text, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< You are a question-answering system that reasons using structured data in the form of triples.\n",
      "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Triples for the reasoning process:\n",
      "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Who is the father of Queen Elizabeth the 2nd?\n",
      "Triples for the reasoning Switch to constrain generation\n",
      "process:\n",
      "triple: <Queen Elizabeth II> <country> <United Kingdom> Switch to normal generation\n",
      ".\n",
      "Switch to constrain generation\n",
      "triple: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[312], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# TODO put tqdm as a streamer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#do_sample = True,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#top_k=3\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:842\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m    853\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:334\u001b[0m, in \u001b[0;36mPhi3Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cache structure has changed since version v4.36. If you are using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor auto-regressive decoding with k/v caching, please make sure to initialize the attention class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a layer index.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[0;32m--> 334\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:156\u001b[0m, in \u001b[0;36mPhi3LongRoPEScaledRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids, seq_len)\u001b[0m\n\u001b[1;32m    154\u001b[0m     ext_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_factor, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     ext_factors \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshort_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m inv_freq_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (ext_factors \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minv_freq_shape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lprocessor = CtrieLogitsProcessor(ctrie=myctrie, initial_state='constrained', switch_pattern=switch_pattern, end_token=newline_token, debug=True)#, tokenizer=tokenizer),\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    lprocessor\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # TODO put tqdm as a streamer\n",
    "    out = model.generate(\n",
    "        input_ids = inputs.input_ids,\n",
    "        output_scores=True,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=500,\n",
    "        streamer = streamer,\n",
    "        #do_sample = True,\n",
    "        #top_k=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9acf2cf3-b9fa-4914-8827-667ef61d6751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Que'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([8654])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c3455fb0-727b-44a5-b62e-def03e4cc0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<s> </<s> < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <</reality>> <has characteristic> <indie game> .'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([529,     1,  1533,     1,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          829,   276,  2877,  6778,   529,  5349, 17443, 29958,   529,   513,\n",
    "          347,  3748, 29958,   869])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1f4f3cc4-49f7-440c-ac7d-2890916385b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2360e908-c1c9-4535-ad4d-dd786ca6a018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29901,   529,     1,  1533,     1,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   829,   276,  2877,  6778,   529,  5349, 17443, 29958,   529,\n",
       "          513,   347,  3748, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "         8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900,\n",
       "        29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900,\n",
       "        29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259,\n",
       "        29958,   869,    13,  3626,   552, 29901,   529,  8654,   264, 13061,\n",
       "          278, 10470, 21869,   313, 29896, 29929, 29900, 29900, 29905, 29884,\n",
       "        29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,\n",
       "         8758,   310, 29958,   529, 29886,   475,  1259, 29958,   869,    13,\n",
       "         3626,   552, 29901,   529,  8654,   264, 13061,   278, 10470, 21869,\n",
       "          313, 29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896,\n",
       "        29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,   310, 29958,\n",
       "          529, 29886,   475,  1259, 29958,   869,    13,  3626,   552, 29901,\n",
       "          529,  8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929,\n",
       "        29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900,\n",
       "        29900, 29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,\n",
       "         1259, 29958,   869,    13,  3626,   552, 29901,   529,  8654,   264,\n",
       "        13061,   278, 10470, 21869,   313, 29896, 29929, 29900, 29900, 29905,\n",
       "        29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,\n",
       "          529,  8758,   310, 29958,   529, 29886,   475,  1259, 29958,   869,\n",
       "           13,  3626,   552, 29901,   529,  8654,   264, 13061,   278, 10470,\n",
       "        21869,   313, 29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900,\n",
       "        29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,   310,\n",
       "        29958,   529, 29886,   475,  1259, 29958,   869,    13,  3626,   552,\n",
       "        29901,   529,  8654,   264, 13061,   278, 10470, 21869,   313, 29896,\n",
       "        29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906,\n",
       "        29900, 29900, 29906, 15410,   529,  8758,   310, 29958,   529, 29886,\n",
       "          475,  1259, 29958,   869,    13,  3626,   552, 29901,   529,  8654,\n",
       "          264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900, 29900,\n",
       "        29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906,\n",
       "        15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259, 29958,\n",
       "          869,    13,  3626,   552, 29901,   529,  8654,   264, 13061,   278,\n",
       "        10470, 21869,   313, 29896, 29929, 29900, 29900, 29905, 29884, 29906,\n",
       "        29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,\n",
       "          310, 29958,   529, 29886,   475,  1259, 29958,   869,    13,  3626,\n",
       "          552, 29901,   529,  8654,   264, 13061,   278, 10470, 21869,   313,\n",
       "        29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941,\n",
       "        29906, 29900, 29900, 29906, 15410,   529,  8758,   310, 29958,   529,\n",
       "        29886,   475,  1259, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "         8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900,\n",
       "        29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900,\n",
       "        29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259,\n",
       "        29958], device='cuda:0')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][len(inputs.input_ids[0])-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "89aa77af-f66d-4428-806b-2cb965310062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a question-answering system that reasons using structured data in the form of triples.\n",
      "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Triples for the reasoning process:\n",
      "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Who is the father of the father of Queen Elizabeth the 2nd?\n",
      "Triples for the reasoning process:\n",
      "triple: <<s> </<s> < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s> < < </<s>  <Unicode block> <Basic Latin> .\n",
      "triple: <Queen Elizabeth the Queen Mother (19\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929068f-39be-42cd-acda-832ec2e09543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4bfa-3829-437f-bdfa-84635b209d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935844e3-47d6-401e-9d80-6c22e6bf171f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c65029-237e-4c91-a890-9f50463d0191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f01d61-ef9d-40b0-b06c-00f392c51dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4d8d-ae1e-40b1-8d84-7acdb88ada61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "48a66043-2df7-4622-bf26-0da8a872199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unconstrained\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # TODO put tqdm as a streamer\n",
    "    out = model.generate(\n",
    "        input_ids = inputs.input_ids,\n",
    "        output_scores=True,\n",
    "        #logits_processor=logits_processor_list,\n",
    "        max_new_tokens=100,\n",
    "        streamer = None,\n",
    "        do_sample = True,\n",
    "        top_k=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd880692-e9b6-49e7-b5ed-e17c592165d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a question-answering system that reasons using structured data in the form of triples.\n",
      "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Triples for the reasoning process:\n",
      "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Which is the capital of Burundi?\n",
      "Triples for the reasoning process:\n",
      "triple: <Burundi> <type> <country>,\n",
      "triple: <Burundi> <capital> <name of capital>,\n",
      "triple: <Kigali> <is capital of> <country> ,\n",
      "triple: <name of capital> <is capital of> <country>,\n",
      "triple: <Kigali> <location> <country>,\n",
      "triple: <Kigali> <location> <region>,\n",
      "triple:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f0029971-9a5b-424b-b361-e07d0be8bc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3626, 552, 29901]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2a5bd8b5-b4b7-4989-862e-e773891a4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3626,   552, 29901,   529, 21140, 29887,  1974, 29958,   529,  5479,\n",
       "        29958,   529, 15654, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "        29940,  1979,  5252, 29958,   529, 11466,   292, 29958,   529, 21140,\n",
       "        29887,  1974, 29958,   869,    13,  3626,   552, 29901,   529, 29931,\n",
       "         1314,  1590, 18041, 29958,   529, 11466,   292, 29958,   529, 21140,\n",
       "        29887,  1974, 29958,   869,    13,  3626,   552, 29901,   529, 29954,\n",
       "          837,  1384, 29958,   529, 11466,   292, 29958,   529, 21140, 29887,\n",
       "         1974, 29958,   869,    13,  3626,   552, 29901,   529, 29909,   504,\n",
       "         2849, 29958,   529, 11466,   292, 29958,   529, 21140, 29887,  1974,\n",
       "        29958,   869,    13,  3626,   552, 29901,   529, 29940,  1979,  5252],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][len(inputs.input_ids[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f10ae26f-6ad2-4531-a7df-047a7816fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Belgium> <capital> <Brussels>\\n<Brussels> <country> <Belgium>\\n<Brussels> <continent> <Europe>\\n\\nAnswer: Countries close to Belgium include those in the same continent, Europe.\\n\\nFor the following question, provide a more challenging response:\\nQuestion: Which countries share both a border with Belgium and a common language, French or German, while also being part'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0][len(inputs.input_ids[0]):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffa6eb42-8af9-4113-b891-15a6de7abad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tri <'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([8602,529])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28c5f4f9-b129-4b4b-ae31-bb6b10a7d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8602, 2701, 363, 278, 24481, 1889, 29901, 13, 29966, 21140, 29887, 1974, 29958, 529, 5030, 2410, 29958, 529, 12432, 1558, 1379, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 13509, 29958, 529, 21140, 29887, 1974, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 1285, 8946, 29958, 529, 15654, 29958], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('''Triples for the reasoning process:\n",
    "<Belgium> <capital> <Brussels>\n",
    "<Brussels> <country> <Belgium>\n",
    "<Brussels> <continent> <Europe>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d2ec2fa-cef5-4e5c-b412-777aa1078c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [29871, 13, 29966, 21140, 29887, 1974, 29958, 529, 5030, 2410, 29958, 529, 12432, 1558, 1379, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 13509, 29958, 529, 21140, 29887, 1974, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 1285, 8946, 29958, 529, 15654, 29958], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('''\n",
    "<Belgium> <capital> <Brussels>\n",
    "<Brussels> <country> <Belgium>\n",
    "<Brussels> <continent> <Europe>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9991f9d-c62c-4dc9-bae9-3b7598d82651",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4b2bc029-da57-4587-95db-0acf153b247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_endswith( seq1, seq2):\n",
    "    if len(seq2) == 0:\n",
    "        return False\n",
    "    subseq1 = seq1[-len(seq2):]\n",
    "    return subseq1 == seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c3d0c64f-2733-4b5e-a020-bd82ba30192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "83207086-31f6-4442-90fa-2bae65298ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_endswith(list(range(10)), [7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab4ab194-8cb3-4f4e-beab-d4db0a8ede46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8758, 2072, 735]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrie.next_tokens([0, 29871, 529])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330a429b-96f5-4cc7-b4b5-048ce35e0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ctrie_Phi-3-mini-128k-instruct.pickle', 'rb') as fd:\n",
    "    ctrie_load = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab56518-c36c-4996-ba7c-1ae6c8495078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529, 18252, 6319, 3705, 1533, 3532, 5277, 15271, 20577, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrie_load.next_tokens([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34102090-05db-4f21-9042-e09ba780caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DonToMPermTw=>41CPtAnydist>JustLSPABLOXAmLABKpeSchHMatFESHETrO'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ctrie_load.next_tokens([18252,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d311b6e6-168f-425d-b0f9-419c86dabeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< <! <? <- </ << <= <> <%<unk>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ctrie_load.next_tokens([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeabf7b-fd69-499e-af62-e29bdbbfb96f",
   "metadata": {},
   "source": [
    "# Da dove arrivano i non '<'???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ba013ff-23d4-48bf-bdd7-74f59e3a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a6e794a3-9d47-41e9-8f7f-c712126f9fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Belouga> <country of registry> <Belize> .'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [529, 21140]\n",
    "for i in range(100):\n",
    "    next_tokens = ctrie_load.next_tokens(seq)\n",
    "\n",
    "    # choice\n",
    "    if next_tokens:\n",
    "        if rand:\n",
    "            chosen_token = random.choice(next_tokens)\n",
    "        else:\n",
    "            chosen_token = next_tokens[0]\n",
    "        \n",
    "\n",
    "        seq.append(chosen_token)\n",
    "    else:\n",
    "        assert ctrie_load.reached_leaf(seq)\n",
    "        break\n",
    "\n",
    "tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eeec6503-dcf9-4f14-b104-295f1cea2b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529, 21140, 346, 29905, 29884, 29900, 29906, 29896, 29929, 2034]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ef976-a62a-4930-b403-290cd2e97904",
   "metadata": {},
   "source": [
    "# Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e5d13639-e0d1-4168-adca-2a2d8d96f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_str = '<Belce\\\\u0219ti> <located in the administrative territorial entity> <Pogone\\\\u0219ti> .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cb832f54-780c-4dc7-bca5-3c0a9cb5c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Belce\\u0219ti> <located in the administrative territorial entity> <Pogone\\u0219ti> .\n"
     ]
    }
   ],
   "source": [
    "print(unicode_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5580dd-59f9-4339-bde7-0ace875113da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Italo Balbo> <allegiance> <Kingdom of Italy> .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([529,\n",
    " 3112,\n",
    " 7003,\n",
    " 7392,\n",
    " 833,\n",
    " 29958,\n",
    " 529,\n",
    " 284,\n",
    " 1397,\n",
    " 8837,\n",
    " 29958,\n",
    " 529,\n",
    " 29968,\n",
    " 292,\n",
    " 3129,\n",
    " 310,\n",
    " 12730,\n",
    " 29958,\n",
    " 869]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050f1883-f89a-4cb8-acbc-67d14d5410fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 529,\n",
       " 29885,\n",
       " 9693,\n",
       " 29958,\n",
       " 529,\n",
       " 2525,\n",
       " 537,\n",
       " 3732,\n",
       " 9324,\n",
       " 29958,\n",
       " 869]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('<Belgium> <motto> <Unity makes strength> .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42480264-43a7-4a9e-ba35-e5203d550f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sujeonggwa> <subclass of> <punch> .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([60000, 529, 2146, 1324, 549, 29887, 2766, 29958, 529, 1491, 1990, 310, 29958, 529, 29886, 3322, 29958, 869])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "750e9cc1-1755-4f99-b3a1-71f650c0e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [60000, 6319, 313, 29943, 6617, 5185, 264, 15410, 529, 689, 689, 310, 907, 1230, 664, 29958, 529, 12073, 3769, 29958, 869, 29958,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f6e511c-46e0-41d0-8224-0b282ae2a3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<? (Fragezeichen)> <formform of creative work> <studio album> .>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(arr[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d243759-c0e2-4f18-ab79-365d36602c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18252, 10310, 1089, 29893, 2353, 29934, 29991, 29958, 529]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83f59363-f68e-4293-add0-ef41d4ee3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "into = tokenizer(\"<Belgium>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a3b6fe7-e27e-4584-bef1-558cc192da26",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m into\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "print(into.pop(0))\n",
    "into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b18a548f-43de-46bf-815b-4ee7f9263a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 529,\n",
       " 13010,\n",
       " 29915,\n",
       " 29879,\n",
       " 1667,\n",
       " 7494,\n",
       " 25792,\n",
       " 29958,\n",
       " 529,\n",
       " 2290,\n",
       " 284,\n",
       " 29901,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 869]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<Belgium> <topic's main Wikimedia portal> <Portal:Belgium> .\")['input_ids']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
