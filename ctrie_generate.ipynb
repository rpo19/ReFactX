{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1aae2a-1466-4d5b-aef4-3f12fb665cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, StoppingCriteria, StoppingCriteriaList, DynamicCache, OffloadedCache #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255d12e2-f799-4897-a15f-d1fdc285c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03167244-77c4-41b6-9698-7d9b412c1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27aeab6-d6d9-47b5-ad49-1357ce197eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_connection = psycopg.connect('postgres://postgres:secret@10.0.0.118:5432/postgres', autocommit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d58190-e8d0-4ee8-b16a-e0b59f9b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4432e09-49df-4af6-a17a-d2348ff2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootkey = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567e0bb8-1be5-4fef-9652-504d64d1c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2feac10f-ab0a-411e-a153-85954dac6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 408, 29, 662]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<end> .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787271bd-c836-4d18-8e35-5d969e1b4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = {'ab':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae52940a-d605-4d7c-aef1-b8ef7b3526e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_triple = 662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0444330-37a1-4a58-97bc-ec68d82e0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.amazonaws', 'ov', '.amazonaws']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([29871, 869, 29871])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0364e3f4-c364-476c-8a35-1986335c8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rootkey > max(tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa01da77-361d-4ace-936e-55f04d335349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a991d2e-78df-48e9-a0b6-19e148cd20c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5cc0fe81064872afeedb6e280dcbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype='bfloat16'\n",
    "            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #trust_remote_code=True,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             #attn_implementation=\"flash_attention_2\",\n",
    "                                             #attn_implementation=\"flash_attention\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6645b9-3be0-4b57-9fcc-138a93e4c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.device.type == device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c2b54b3-6c0d-45ee-abdd-dff9f66718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': '''You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "5) You provide a short concise answer.\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '''Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': '''\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "'''\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ec60bee-3eba-4cc2-9c66-d5595d27500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\\n1) You receive an input question.\\n3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\\n5) You provide a short concise answer.\\n'}, {'role': 'user', 'content': 'Which mountain is taller between Mont Blanc and Mount Rainier?\\n'}, {'role': 'assistant', 'content': '\\nFact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\\nFact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\\n\\nAnswer: Mont Blanc.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f340e2-8353-4f43-98d6-7e6c82bd4f2a",
   "metadata": {},
   "source": [
    "## Find switch pattern\n",
    "may be tokenizer dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27832621-8b67-4a05-9fa6-251000ed2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 17873, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ċ', 'Fact', ':']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern = tokenizer('''\n",
    "Fact:''').input_ids\n",
    "print(switch_pattern)\n",
    "tokenizer.convert_ids_to_tokens(switch_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0a7d17f-72f6-4c4f-a27f-90a7edb82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_pattern = [17873, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d9fa421-7175-4aad-b9a2-a3c5b56d0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import PostgresTrieIndex, ConstrainedLogitsProcessor, ConstrainedStateList, GetAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d08c2aa0-5493-4e5d-937a-322c5574f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_token = tokenizer('''\n",
    "''').input_ids[-1]\n",
    "newline_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e309ab38-4878-4551-89d7-aa466d13b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "myctrie = PostgresTrieIndex(rootkey=rootkey, postgresql_connection=postgresql_connection, switch_parameter=6, table_name='ctriev2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97d9c5e3-f4ad-4953-b297-5f8acd05c10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100265"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {'role':'user', 'content': '''Which city is the capital of the country where the Eiffel Tower is?\n",
    "'''}\n",
    "question = {'role':'user', 'content': '''Who was the quarterback of the team that won Super Bowl 50?\n",
    "'''}\n",
    "prompted_texts = [tokenizer.apply_chat_template(prompt + [question], tokenize=False, add_generation_prompt=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "03db7b6e-5c68-46db-b3c1-019c3e5d7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "5) You provide a short concise answer.\n",
      "<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|><|im_start|>assistant<|im_sep|>\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|><|im_start|>user<|im_sep|>Who was the quarterback of the team that won Super Bowl 50?\n",
      "<|im_end|><|im_start|>assistant<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "print(prompted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0e95dd90-3572-4a64-9ffb-f270247b2bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ba821415-8673-44ad-9143-7d29dd141bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ċ'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.convert_ids_to_tokens(newline_token) # tokenizer.tokenize('\\n')[0]\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompted_texts, return_tensors='pt', padding=True, padding_side='right')\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c7e38b57-f1ab-4890-93a8-cbc7b1e78a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100264,   9125, 100266,   2675,    527,    264,  11190,   3488,  36864,\n",
       "          18328,    430,  23963,   1202,   4320,    389,  13363,    505,    264,\n",
       "           6677,   2385]], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "93992ae3-7120-4750-8f48-a92afa3335ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "constrained_processor = ConstrainedLogitsProcessor(index=myctrie, switch_pattern=switch_pattern, end_token=newline_token)#, tokenizer=tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "602c685e-da3e-4a25-9dfd-25d13664b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 16533, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16533, 25]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode('''\n",
    "Answer:'''))\n",
    "answer_tokens = [16533, 25]\n",
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4f759180-6a93-413f-a4bb-bbaa3487b894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[366, 60704, 29]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' <Paris>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2cb3c92f-e187-4a7a-bba7-4f5337b23d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ<'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "894ebe31-131c-42dc-9e20-5407f42d6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_parentheses_right = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bb0169e1-411f-4b9b-9812-b7d80a5c3e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100265"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "57a042e4-f2e5-4b96-8063-d30721ccd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "getanswer = GetAnswer(answer_tokens, [newline_token, tokenizer.eos_token_id], all)\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    getanswer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "045a725d-8d4d-45e1-bbcb-981df8f943b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16533, 25]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5f3454a9-9f45-4f61-8b27-09979b7e842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 149])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4af470ad-8a6e-4ceb-8447-73747aedfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pdb.Pdb().set_break('/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py', 3586)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "5) You provide a short concise answer.<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|><|im_start|>assistant<|im_sep|>Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "\n",
      "50?<|im_end|><|im_start|>assistant<|im_sep|>r<|im_sep|>Who was the quarterback of the team that won Super Bowl \n",
      "Fact:<The Denver Post> <headquarters location> <Denver> .\n",
      "Fact:<Denver Broncos> <home city> <Denver> .\n",
      "Fact:<Super Bowl 50> <winning team> <Denver Broncos> .\n",
      "Fact:<Super Bowl 50> <date> <February 7, 2016> .\n",
      "Fact:<Peyton Manning> <position> <quarterback> .\n",
      "Fact:<Peyton Manning> <team during Super Bowl 50> <Denver Broncos> .\n",
      "\n",
      "Answer: Peyton Manning.<|im_end|>\n",
      "Elapsed 16.6826434135437\n"
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "states = ConstrainedStateList(num_beams, switch_pattern, newline_token)\n",
    "\n",
    "constrained_processor = ConstrainedLogitsProcessor(index=converter_index, end_token=newline_token, states=states)#, tokenizer=tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    getanswer.set_prompt(inputs.input_ids[0])\n",
    "\n",
    "    genargs = dict(\n",
    "        **inputs,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=200,\n",
    "        streamer = streamer,\n",
    "        #do_sample = True,\n",
    "        #top_k=3,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=1,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        kwargs = {'constrained_state': states}, # passing state\n",
    "    )\n",
    "    out = model.generate(**genargs)\n",
    "    #pdb.runcall(model.generate, None, **genargs)\n",
    "print('Elapsed', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "89aa77af-f66d-4428-806b-2cb965310062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ tensor(1561558, device='cuda:0') 109\n",
      "\n",
      "Fact:<The Denver Post> <headquarters location> <Denver> .\n",
      "Fact:<Denver Broncos> <home city> <Denver> .\n",
      "Fact:<Super Bowl 50> <winning team> <Denver Broncos> .\n",
      "Fact:<Super Bowl 50> <date> <February 7, 2016> .\n",
      "Fact:<Peyton Manning> <position> <quarterback> .\n",
      "Fact:<Peyton Manning> <team during Super Bowl 50> <Denver Broncos> .\n",
      "\n",
      "Answer: Peyton Manning.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print('-'*30, sum(out[i][len(inputs.input_ids[0]):]), len(out[i][len(inputs.input_ids[0]):]))\n",
    "    print(tokenizer.decode(out[i][len(inputs.input_ids[0]):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "072f86f6-d4aa-465c-8ad9-82f467175679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "5) You provide a short concise answer.<|im_end|><|im_start|>user<|im_sep|>Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|><|im_start|>assistant<|im_sep|>Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "\n",
      "Answer: Mont Blanc.<|im_end|><|im_start|>user<|im_sep|>Who was the quarterback of the team that won Super Bowl 50?<|im_end|><|im_start|>assistant<|im_sep|>\n",
      "Fact:<The Denver Post> <headquarters location> <Denver> .\n",
      "Fact:<Denver Broncos> <home city> <Denver> .\n",
      "Fact:<Super Bowl 50> <winning team> <Denver Broncos> .\n",
      "Fact:<Super Bowl 50> <date> <February 7, 2016> .\n",
      "Fact:<Peyton Manning> <position> <quarterback> .\n",
      "Fact:<Peyton Manning> <team during Super Bowl 50> <Denver Broncos> .\n",
      "\n",
      "Answer: Peyton Manning.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print(tokenizer.decode(out[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cc028-f2f6-4402-8232-94b990bcc613",
   "metadata": {},
   "source": [
    "<|im_start|>system<|im_sep|>\n",
    "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "5) You provide a short concise answer.\n",
    "<|im_end|><|im_start|>user<|im_sep|>\n",
    "\n",
    "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "\n",
    "Answer: Mont Blanc.<|im_end|><|im_start|>user<|im_sep|>\n",
    "\n",
    "Which city is the capital of the country where the Eiffel Tower is?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact:<Eiffel Tower> <location> <Champ de Mars> .\n",
    "Fact:<Champ de Mars> <city> <Paris> .\n",
    "Fact:<Paris> <capital of> <France> .\n",
    "\n",
    "Answer: Paris.<|im_end|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea1988-5b04-47f8-99f7-9512768ae532",
   "metadata": {},
   "source": [
    "Who was the quarterback of the team that won Super Bowl 50?\n",
    "<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "\n",
    "Fact:<The Denver Post> <headquarters location> <Denver> .\n",
    "Fact:<Denver Broncos> <home city> <Denver> .\n",
    "Fact:<Super Bowl 50> <winning team> <Denver Broncos> .\n",
    "Fact:<Super Bowl 50> <date> <February 7, 2016> .\n",
    "Fact:<Peyton Manning> <position> <quarterback> .\n",
    "Fact:<Peyton Manning> <team during Super Bowl 50> <Denver Broncos> .\n",
    "\n",
    "Answer: Peyton Manning.<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9631cc0d-16d8-4d61-abeb-877401641bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, ' Paris.<|im_end|>')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop, ans = getanswer.get_answer(out[0])\n",
    "# TODO: is getanswer still needed?\n",
    "stop, tokenizer.decode(list(ans)) # TODO trim and remove special tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
