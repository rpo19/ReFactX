{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741e03e3-91ba-48c5-9f20-6ba1ee145dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bz2\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "import psycopg\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, StoppingCriteria, StoppingCriteriaList, DynamicCache, OffloadedCache #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1878915d-e1ab-41e0-8e74-5566d33184d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remind to remove '.py'\n",
    "# index_module = importlib.import_module('phi4_index')\n",
    "#index_module = importlib.import_module('http_index')\n",
    "index_module = importlib.import_module('qwen25_index')\n",
    "#index_module = importlib.import_module('gemma3_index')\n",
    "#index_module = importlib.import_module('llama_index')\n",
    "index_config = getattr(index_module, 'index_config')\n",
    "\n",
    "# model_module = importlib.import_module('phi4_model')\n",
    "model_module = importlib.import_module('qwen25_1B_model')\n",
    "#model_module = importlib.import_module('gemma3_4B_model')\n",
    "#model_module = importlib.import_module('llama_1B_model')\n",
    "model_config = getattr(model_module, 'model_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(model_config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5404188-5b5f-4952-9ad1-24d4198bfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import ConstrainedLogitsProcessor, ConstrainedStateList, \\\n",
    "                    ConstrainedState, DictIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Which city is the capital of the country where the Eiffel Tower is?'''\n",
    "question = '''Who was the quarterback of the team that won Super Bowl 50?'''\n",
    "question = '''Who is the lead character in the Uncharted series?'''\n",
    "question = '''Who played Gandalf in the Lord of the Rings movies?'''\n",
    "question = '''What was Nirvana's last studio album called?''' # TODO test with beamsearch 3. with beam search if I have only 1 allowed token the other beams could generate something illegal and than crash\n",
    "# TODO understand the crash is due to beam search and abandon those illegal paths. or try to explore always different triples with beam search???\n",
    "\n",
    "\n",
    "question = '''How many members were there in the original NSYNC music group?'''\n",
    "\n",
    "question = 'Who influenced Nirvana band?'\n",
    "\n",
    "question = 'Is Johnny Depp or Brad Pitt older?'\n",
    "#question = 'Who is the mother of the director of film Polish-Russian War (Film)?' # Ma≈Çgorzata Braunek\n",
    "#question = 'Which film came out first, Blind Shaft or The Mask Of Fu Manchu?' # The Mask Of Fu Manchu\n",
    "question = 'When did John V, Prince Of Anhalt-Zerbst\\'s father die?' # 12 June 1516\n",
    "#question = 'Is Johnny Depp older than Brad Pitt?'\n",
    "\n",
    "question = '''During which years did Napoleon hold power in France?'''\n",
    "question = '''When was Barack Obama president?'''\n",
    "\n",
    "prompted_texts = [model_config.apply_prompt_template(question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03db7b6e-5c68-46db-b3c1-019c3e5d7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful question-answering assistant that bases its answers on facts from a knowledge base and always respects the prompt.\n",
      "\n",
      "Process:\n",
      "\n",
      "    You receive an input question.\n",
      "\n",
      "    You get some context about the entities in the questions using description and short description.\n",
      "\n",
      "    You determine the reasoning path needed to answer the question based on the information available.\n",
      "\n",
      "    You explicitly list relevant facts, one per line, starting with \"Fact:\".\n",
      "\n",
      "    You explain your reasoning step by step and provide a detailed answer, justifying it based on the facts.\n",
      "\n",
      "    You conclude with a short and concise answer that must be grounded by the facts you found.\n",
      "\n",
      "You have to answer only basing on the facts you find in the knowledge base.\n",
      "If you cannot find relevant facts in the knowledge base to support an answer you stop and you reply: \"I don't know.\".\n",
      "After generating facts, please reason on top of the facts you find and avoid generating lot of useless facts.\n",
      "\n",
      "You must always follow these instructions precisely and ensure your responses adhere strictly to this prompt.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Let's try to get a bit of context about these two mountains starting with the descriptions.\n",
      "Fact: <Mont Blanc> <description> <highest mountain in the Alps> .\n",
      "I found that Mont Blanc is the highest mountain in the Alps, but I didn't find its height.\n",
      "Fact: <Mount Rainier> <description> <stratovolcano in the U.S. state of Washington> .\n",
      "I found that Mount Rainier is a stratovolcano and its location. But I didn't find its height.\n",
      "Fact: <Mont Blanc> <short description> <Highest mountain in the Alps (4,808 m)> .\n",
      "I found the height of Mont Blanc.\n",
      "Fact: <Mount Rainier> <short description> <Stratovolcano in the U.S. state of Washington> .\n",
      "I found again that Mount Rainier is a stratovolcano, but I still need to find its height.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "I found Mount Rainier elevation above sea level.\n",
      "Now that I know the height of both mountains I can give a grounded answer.\n",
      "Long answer: Mont Blanc is 4,808 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.<|im_end|>\n",
      "<|im_start|>user\n",
      "When was the director of Slumdog Millionaire born?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the birth date of the director of Slumdog Millionaire. Let's start with Slumdog Millionaire description.\n",
      "Fact: <Slumdog Millionaire> <description> <2008 film directed by Danny Boyle> .\n",
      "I found that the director of Slumdog Millionaire is Danny Boyle. Now I need the birth date of Danny Boyle.\n",
      "Fact: <Danny Boyle> <date of birth> <1956-10-20T00:00:00Z>.\n",
      "Now I have all the information I need for answering.\n",
      "Long answer: The director of Slumdog Millionaire is Danny Boyle, who was born on October 20, 1956, so this is the answer to the question.\n",
      "\n",
      "Answer: October 20, 1956.<|im_end|>\n",
      "<|im_start|>user\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find who the father of John V, Prince of Anhalt-Zerbst, was and then find the date of his death. Let's start with the description.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <description> <Prince of Anhalt-Dessau and Anhalt-Zerbst> .\n",
      "Ok, I still need to find his father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Ernest I, Prince of Anhalt-Dessau> .\n",
      "Now I know who is the father. I need to find when he died.\n",
      "Fact: <Ernest I, Prince of Anhalt-Dessau> <date of death> <1516-06-12T00:00:00Z> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: The father of John V, Prince of Anhalt-Zerbst, Ernest I, Prince of Anhalt-Dessau, died on June 12, 1516.\n",
      "\n",
      "Answer: June 12, 1516.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp older than Brad Pitt?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine if Johnny Depp is older than Brad Pitt, I need to find their respective birth years. Let's start with the description.\n",
      "Fact: <Johnny Depp> <description> <American actor (born 1963)> .\n",
      "I found the year of birth of Johnny Depp. I need Brad Pitt's one.\n",
      "Fact: <Brad Pitt> <description> <American actor and filmmaker> .\n",
      "I cannot find Brad Pitt date of birth in the description.\n",
      "Fact: <Brad Pitt> <short description> <American actor (born 1963)> .\n",
      "I found Brad Pitt's year of birth. But since it is the same as Johnny Depp's, I need more precise information about their date of birth.\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "I found when Johnny Depp was born, now I need Brad Pitt's date of birth.\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Johnny Depp was born on June 9, 1963, while Brad Pitt was born on December 18, 1963. So yes, Johnny Depp is older than Brad Pitt.\n",
      "\n",
      "Answer: Yes.<|im_end|>\n",
      "<|im_start|>user\n",
      "During which years did Napoleon hold power in France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine the years Napoleon held power in France, I need to identify the periods during which he held official leadership roles such as First Consul or Emperor. Maybe I can get this information from the description.\n",
      "Fact: <Napoleon Bonaparte> <description> <French military leader, French Emperor 1804‚Äì1814 and again in 1815> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Based on the description, Napoleon Bonaparte held power in France as Emperor from 1804 to 1814, and briefly again in 1815.\n",
      "\n",
      "Answer: From 1799 to 1814, and in 1815.<|im_end|>\n",
      "<|im_start|>user\n",
      "In which books of A Song of Ice and Fire is Jon Snow not the Lord Commander of the Night's Watch?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to consider the books in the A Song of Ice and Fire series by George R.R. Martin and identify the ones where Jon Snow is not the Lord Commander of the Night's Watch. Let's start with the description.\n",
      "Fact: <A Game of Thrones> <description> <1996 novel by George R. R. Martin> .\n",
      "This fact doesn't give me any information about Jon Snow.\n",
      "Fact: <A Game of Thrones> <publication date> <1996-08-06T00:00:00Z> .\n",
      "This fact is not giving any information about Jon Snow. I think the publication date won't help me.\n",
      "Fact: <A Clash of Kings> <publication date> <1998-11-16T00:00:00Z> .\n",
      "These facts don't contain any information about Jon Snow. I'll try one more time.\n",
      "Fact: <A Storm of Swords> <publication date> <2000-08-08T00:00:00Z> .\n",
      "I'm not able to find the facts about the books in which Jon Snow is not the Lord Commander, so I cannot give a grounded answer.\n",
      "Long answer: I don't know. I was not able to find the relevant facts in the knowledge base to motivate an answer.\n",
      "\n",
      "Answer: I don't know.<|im_end|>\n",
      "<|im_start|>user\n",
      "When was Barack Obama president?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model_config.tokenizer(prompted_texts, return_tensors='pt', padding=True, padding_side='right')\n",
    "inputs = inputs.to(model_config.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6d4668-a82c-4089-bee9-998051c72447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928bc065-deb5-4f17-b108-691b210eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_8925/4180010291.py\u001b[39m(\u001b[92m29\u001b[39m)\u001b[36m<module>\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[32m     28\u001b[39m \n",
      "\u001b[32m---> 29\u001b[39m     pdb.set_trace() \u001b[38;5;66;03m# b /opt/conda/envs/development/lib/python3.13/site-packages/transformers/generation/utils.py:3984\u001b[39;00m\n",
      "\u001b[32m     30\u001b[39m     out = model_config.model.generate(\n",
      "\u001b[32m     31\u001b[39m         **inputs,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "1   breakpoint   keep yes   at /opt/conda/envs/development/lib/python3.13/site-packages/transformers/generation/utils.py:3984\n",
      "\tbreakpoint already hit 1 time\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b /workspace/notebooks/secondment/ctrie.py:687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 2 at /workspace/notebooks/secondment/ctrie.py:687\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/opt/conda/envs/development/lib/python3.13/site-packages/transformers/generation/utils.py\u001b[39m(\u001b[92m3984\u001b[39m)\u001b[36m_beam_search\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m   3982\u001b[39m                 num_beams=num_beams,\n",
      "\u001b[32m   3983\u001b[39m             )\n",
      "\u001b[91m1\u001b[39m\u001b[32m> 3984\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'constrained_state'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m model_kwargs.get(\u001b[33m'kwargs'\u001b[39m, {}):\n",
      "\u001b[32m   3985\u001b[39m                 model_kwargs[\u001b[33m'kwargs'\u001b[39m][\u001b[33m'constrained_state'\u001b[39m].beam_scores = running_beam_scores\n",
      "\u001b[32m   3986\u001b[39m                 model_kwargs[\u001b[33m'kwargs'\u001b[39m][\u001b[33m'constrained_state'\u001b[39m].beam_next_tokens = running_sequences\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/workspace/notebooks/secondment/ctrie.py\u001b[39m(\u001b[92m687\u001b[39m)\u001b[36mbeam_permutation\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m beam_permutation(self):\n",
      "\u001b[32m    686\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(self.beam_idx) > \u001b[32m0\u001b[39m: \u001b[38;5;66;03m# ignore first call\u001b[39;00m\n",
      "\u001b[91m2\u001b[39m\u001b[32m-> 687\u001b[39m             \u001b[38;5;28;01massert\u001b[39;00m len(self.beam_idx) == self.num_beams * self.batch_size, \u001b[33m'ERROR: beam_idx size unexpected.'\u001b[39m\n",
      "\u001b[32m    688\u001b[39m             copies = [state.dump(copy=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;28;01min\u001b[39;00m self.states]\n",
      "\u001b[32m    689\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i, (state, beam_i) \u001b[38;5;28;01min\u001b[39;00m enumerate(zip(self.states, self.beam_idx)):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p self.beam_idx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, -1, -1,  ..., -1, -1, -1],\n",
      "         [ 0, -1, -1,  ..., -1, -1, -1],\n",
      "         [ 0, -1, -1,  ..., -1, -1, -1]]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p len(self.beam_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p self.num_beams * self.batch_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.beam_idx.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.beam_idx[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, -1, -1,  ..., -1, -1, -1],\n",
      "        [ 0, -1, -1,  ..., -1, -1, -1],\n",
      "        [ 0, -1, -1,  ..., -1, -1, -1]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.beam_idx[0][0,:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.beam_idx[0][:,0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "num_beams = 3\n",
    "\n",
    "auto_streamer = streamer if num_beams == 1 else None\n",
    "\n",
    "states = ConstrainedStateList(\n",
    "    [ConstrainedState(\n",
    "                begin_pattern = model_config.switch_pattern,\n",
    "                end_pattern = model_config.newline_token,\n",
    "                cache_index = DictIndex(end_of_triple=index_config.index.end_of_triple),\n",
    "                subtree_cache = DictIndex(end_of_triple=index_config.index.end_of_triple),\n",
    "                oneleaf_cache = DictIndex(end_of_triple=index_config.index.end_of_triple)\n",
    "            ) for _ in range(num_beams)],\n",
    "            num_beams=num_beams,\n",
    "            batch_size = 1,\n",
    "            pad_token_id = model_config.tokenizer.eos_token_id)\n",
    "\n",
    "constrained_processor = ConstrainedLogitsProcessor(\n",
    "    index=index_config.index,\n",
    "    end_token=model_config.newline_token, states=states, tokenizer=model_config.tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])\n",
    "\n",
    "model_config.model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    pdb.set_trace() # b /opt/conda/envs/development/lib/python3.13/site-packages/transformers/generation/utils.py:3984\n",
    "    out = model_config.model.generate(\n",
    "        **inputs,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=800,\n",
    "        streamer = auto_streamer,\n",
    "        do_sample = False,\n",
    "        temperature = None,\n",
    "        top_k=None,\n",
    "        num_beams=num_beams,\n",
    "        #num_beam_groups = num_beams // 2,\n",
    "        ## TODO group beam search works differently with logitsprocessors: input ids shape is group shape = num_beams / num_beam_groups\n",
    "        #diversity_penalty=0.5,\n",
    "        #num_return_sequences=1,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        #stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        top_p=None,\n",
    "        min_p=None,\n",
    "        #past_key_values=past_key_values,\n",
    "        kwargs = {'constrained_state': states}, # passing state\n",
    "    )\n",
    "\n",
    "print('Elapsed', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d2fec-5a45-47d2-9e0c-630ddb969bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug for updating to transformers 4.51\n",
    "'''1   breakpoint   keep yes   at /opt/conda/envs/development/lib/python3.13/site-packages/transformers/generation/utils.py:3984\n",
    "\tbreakpoint already hit 1 time\n",
    "\n",
    "ipdb>  b /workspace/notebooks/secondment/ctrie.py:687\n",
    "\n",
    "ipdb>  p self.beam_idx\n",
    "\n",
    "tensor([[[ 0, -1, -1,  ..., -1, -1, -1],\n",
    "         [ 0, -1, -1,  ..., -1, -1, -1],\n",
    "         [ 0, -1, -1,  ..., -1, -1, -1]]], device='cuda:0', dtype=torch.int32)\n",
    "\n",
    "ipdb>  p len(self.beam_idx)\n",
    "\n",
    "1\n",
    "\n",
    "ipdb>  p self.num_beams * self.batch_size\n",
    "\n",
    "3\n",
    "\n",
    "ipdb>  self.beam_idx.shape\n",
    "\n",
    "torch.Size([1, 3, 800]) 1 batch_size, 3 num_beams, 800 max_new_tokens # verifica nel codice e empiricamente\n",
    "\n",
    "# todo capire a che generazione siamo e prendere gli indici cos√¨\n",
    "self.beam_idx[batch_num][:,generation_num]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e657fa51-3673-4ac3-9349-4bb817b7a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a1b8386-fcbc-4bf3-b13b-e388f4bbc537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ tensor(492040, device='cuda:0') 117\n",
      "Reasoning: To answer this question, I need to find the start and end years of Barack Obama's presidency. Let's start with the description.\n",
      "Fact: <Barack Obama> <description> <president of the United States from 2009 to 2017> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Barack Obama served as the President of the United States from 2009 to 2017.\n",
      "\n",
      "Answer: 2009 to 2017.<|im_end|>\n",
      "------------------------------ tensor(633566, device='cuda:0') 117\n",
      "Reasoning: To answer this question, I need to find the start and end years of Barack Obama's presidency. Let's start with the description.\n",
      "Fact: <Barack Obama> <description> <president of the United States from 2009 to 2017> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Barack Obama was the president of the United States from 2009 to 2017.\n",
      "\n",
      "Answer: 2009 to 2017.<|im_end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "_from = len(inputs.input_ids[0]) # 0\n",
    "for i in range(out.shape[0]):\n",
    "    print('-'*30, sum(out[i][_from:]), len(out[i][_from:]))\n",
    "    print(model_config.tokenizer.decode(out[i][_from:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dfb7cc7c-ed8d-4b48-9e97-c12b63641329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  <Barack Obama> <position held> <President of the United States> . [366, 3428, 473, 7105, 29, 366, 3487, 5644, 29, 366, 26137, 315, 279, 3639, 4180, 29, 659, 198]\n",
      "1  <Barack Obama> <date of birth> <1961-08-04T00:00:00Z> . [366, 3428, 473, 7105, 29, 366, 1028, 315, 7194, 29, 366, 16, 24, 21, 16, 12, 15, 23, 12, 15, 19, 51, 15, 15, 25, 15, 15, 25, 15, 15, 57, 29, 659, 198]\n",
      "2  <Barack Obama> <significant event> <First inauguration of Barack Obama> . [366, 3428, 473, 7105, 29, 366, 90545, 1538, 29, 366, 5338, 59710, 315, 23539, 7105, 29, 659, 198]\n",
      "3  <Barack Obama> <significant event> <Second inauguration of Barack Obama> . [366, 3428, 473, 7105, 29, 366, 90545, 1538, 29, 366, 15666, 59710, 315, 23539, 7105, 29, 659, 198]\n"
     ]
    }
   ],
   "source": [
    "for i, triple in enumerate(states[0].generated_triples):\n",
    "    print(i, model_config.tokenizer.decode(triple)[:-1], triple, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb4360-f630-43b5-9a7a-dac741ca9e77",
   "metadata": {},
   "source": [
    "# Debug unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db609feb-4a3c-4194-bd01-8327312e30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct\n",
      "<|im_start|>system\n",
      "You are a helpful question-answering assistant that bases its answers on facts from a knowledge base and always respects the prompt.\n",
      "\n",
      "Process:\n",
      "\n",
      "    You receive an input question.\n",
      "\n",
      "    You determine the reasoning path needed to answer the question based on the information available.\n",
      "\n",
      "    You explicitly list relevant facts, one per line, starting with \"Fact:\".\n",
      "\n",
      "    You explain your reasoning step by step and provide a detailed answer, justifying it based on the facts.\n",
      "\n",
      "    You conclude with a short and concise answer.\n",
      "\n",
      "Handling missing facts:\n",
      "\n",
      "    If no relevant facts are found in the knowledge base, you:\n",
      "    a) Provide an answer based on general knowledge, explicitly stating that no supporting facts were found.\n",
      "    b) If you do not know the answer or are not confident enough, simply reply: \"I don't know.\"\n",
      "\n",
      "You must always follow these instructions precisely and ensure your responses adhere strictly to this prompt.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02¬±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When was the director of Slumdog Millionaire born?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the birth date of the director of Slumdog Millionaire.\n",
      "Fact: <Slumdog Millionaire> <director> <Danny Boyle>.\n",
      "Now I need the birth date of Danny Boyle.\n",
      "Fact: <Danny Boyle> <date of birth> <1956-10-20T00:00:00Z>.\n",
      "Long answer: The director of Slumdog Millionaire is Danny Boyle, who was born on October 20, 1956, so this is the answer to the question.\n",
      "\n",
      "Answer: October 20, 1956.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is married to Tim Allen?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find information about Tim Allen's spouse.\n",
      "Fact: <Tim Allen> <spouse> <Jane Hajduk (American actress)>.\n",
      "Long answer: I found that Tim Allen is married to Jane Hajduk.\n",
      "\n",
      "Answer: Jane Hajduk.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp older than Brad Pitt?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine if Johnny Depp is older than Brad Pitt, I need to find their respective birth years.\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Long answer: Johnny Depp was born on June 9, 1963, while Brad Pitt was born on December 18, 1963. So yes, Johnny Depp is older than Brad Pitt.\n",
      "\n",
      "Answer: Yes.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the death date of John V, Prince of Anhalt-Zerbst's father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Frederick II, Duke of Saxe-Coburg-Saalfeld>.\n",
      "Now I need the death date of Frederick II, Duke of Saxe-Coburg-Saalfeld.\n",
      "Fact: <Frederick II, Duke of Saxe-Coburg-Saalfeld> <date of death> <1773-06-15T00:00:00Z>.\n",
      "Long answer: John V, Prince of Anhalt-Zerbst's father died on June 15, 1773.\n",
      "\n",
      "Answer: June 15, 1773.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "auto_streamer = streamer if num_beams == 1 else None\n",
    "\n",
    "model_config.model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    out = model_config.model.generate(\n",
    "        **inputs,\n",
    "        #logits_processor=logits_processor_list,\n",
    "        max_new_tokens=800,\n",
    "        streamer = auto_streamer,\n",
    "        do_sample = False,\n",
    "        temperature = 0.0,\n",
    "        top_k=None,\n",
    "        num_beams=num_beams,\n",
    "        #num_beam_groups = num_beams // 2,\n",
    "        ## TODO group beam search works differently with logitsprocessors: input ids shape is group shape = num_beams / num_beam_groups\n",
    "        #diversity_penalty=0.5,\n",
    "        #num_return_sequences=1,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        #stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        #kwargs = {'constrained_state': states}, # passing state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db4435-ec16-4647-a100-d29d990de00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
