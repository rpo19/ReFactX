{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741e03e3-91ba-48c5-9f20-6ba1ee145dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bz2\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "import psycopg\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, StoppingCriteria, StoppingCriteriaList, DynamicCache, OffloadedCache #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1878915d-e1ab-41e0-8e74-5566d33184d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "### Remind to remove '.py'\n",
    "# index_module = importlib.import_module('phi4_index')\n",
    "#index_module = importlib.import_module('http_index')\n",
    "index_module = importlib.import_module('qwen25_index')\n",
    "#index_module = importlib.import_module('gemma3_index')\n",
    "#index_module = importlib.import_module('llama_index')\n",
    "index_config = getattr(index_module, 'index_config')\n",
    "\n",
    "# model_module = importlib.import_module('phi4_model')\n",
    "model_module = importlib.import_module('qwen25_1B_model')\n",
    "#model_module = importlib.import_module('gemma3_4B_model')\n",
    "#model_module = importlib.import_module('llama_1B_model')\n",
    "model_config = getattr(model_module, 'model_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(model_config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5404188-5b5f-4952-9ad1-24d4198bfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import ConstrainedLogitsProcessor, ConstrainedStateList, \\\n",
    "                    ConstrainedState, DictIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Which city is the capital of the country where the Eiffel Tower is?'''\n",
    "question = '''Who was the quarterback of the team that won Super Bowl 50?'''\n",
    "question = '''Who is the lead character in the Uncharted series?'''\n",
    "question = '''Who played Gandalf in the Lord of the Rings movies?'''\n",
    "question = '''What was Nirvana's last studio album called?''' # TODO test with beamsearch 3. with beam search if I have only 1 allowed token the other beams could generate something illegal and than crash\n",
    "# TODO understand the crash is due to beam search and abandon those illegal paths. or try to explore always different triples with beam search???\n",
    "\n",
    "\n",
    "question = '''How many members were there in the original NSYNC music group?'''\n",
    "\n",
    "question = 'Who influenced Nirvana band?'\n",
    "\n",
    "question = 'Is Johnny Depp or Brad Pitt older?'\n",
    "#question = 'Who is the mother of the director of film Polish-Russian War (Film)?' # Małgorzata Braunek\n",
    "#question = 'Which film came out first, Blind Shaft or The Mask Of Fu Manchu?' # The Mask Of Fu Manchu\n",
    "question = 'When did John V, Prince Of Anhalt-Zerbst\\'s father die?' # 12 June 1516\n",
    "#question = 'Is Johnny Depp older than Brad Pitt?'\n",
    "\n",
    "prompted_texts = [model_config.apply_prompt_template(question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03db7b6e-5c68-46db-b3c1-019c3e5d7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful question-answering assistant that bases its answers on facts from a knowledge base and always respects the prompt.\n",
      "\n",
      "Process:\n",
      "\n",
      "    You receive an input question.\n",
      "\n",
      "    You determine the reasoning path needed to answer the question based on the information available.\n",
      "\n",
      "    You explicitly list relevant facts, one per line, starting with \"Fact:\".\n",
      "\n",
      "    You explain your reasoning step by step and provide a detailed answer, justifying it based on the facts.\n",
      "\n",
      "    You conclude with a short and concise answer.\n",
      "\n",
      "Handling missing facts:\n",
      "\n",
      "    If no relevant facts are found in the knowledge base, you:\n",
      "    a) Provide an answer based on general knowledge, explicitly stating that no supporting facts were found.\n",
      "    b) If you do not know the answer or are not confident enough, simply reply: \"I don't know.\"\n",
      "\n",
      "You must always follow these instructions precisely and ensure your responses adhere strictly to this prompt.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When was the director of Slumdog Millionaire born?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the birth date of the director of Slumdog Millionaire.\n",
      "Fact: <Slumdog Millionaire> <director> <Danny Boyle>.\n",
      "Now I need the birth date of Danny Boyle.\n",
      "Fact: <Danny Boyle> <date of birth> <1956-10-20T00:00:00Z>.\n",
      "Long answer: The director of Slumdog Millionaire is Danny Boyle, who was born on October 20, 1956, so this is the answer to the question.\n",
      "\n",
      "Answer: October 20, 1956.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is married to Tim Allen?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find information about Tim Allen's spouse.\n",
      "Fact: <Tim Allen> <spouse> <Jane Hajduk (American actress)>.\n",
      "Long answer: I found that Tim Allen is married to Jane Hajduk.\n",
      "\n",
      "Answer: Jane Hajduk.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp older than Brad Pitt?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine if Johnny Depp is older than Brad Pitt, I need to find their respective birth years.\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Long answer: Johnny Depp was born on June 9, 1963, while Brad Pitt was born on December 18, 1963. So yes, Johnny Depp is older than Brad Pitt.\n",
      "\n",
      "Answer: Yes.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model_config.tokenizer(prompted_texts, return_tensors='pt', padding=True, padding_side='right')\n",
    "inputs = inputs.to(model_config.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6d4668-a82c-4089-bee9-998051c72447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928bc065-deb5-4f17-b108-691b210eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gemma3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gemma3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 34.7655770778656\n"
     ]
    }
   ],
   "source": [
    "num_beams = 3\n",
    "\n",
    "auto_streamer = streamer if num_beams == 1 else None\n",
    "\n",
    "states = ConstrainedStateList(\n",
    "    [ConstrainedState(\n",
    "                begin_pattern = model_config.switch_pattern,\n",
    "                end_pattern = model_config.newline_token,\n",
    "                cache_index = DictIndex(end_of_triple=index_config.index.end_of_triple),\n",
    "                subtree_cache = DictIndex(end_of_triple=index_config.index.end_of_triple),\n",
    "                oneleaf_cache = DictIndex(end_of_triple=index_config.index.end_of_triple)\n",
    "            ) for _ in range(num_beams)],\n",
    "            num_beams=num_beams,\n",
    "            batch_size = 1,\n",
    "            pad_token_id = model_config.tokenizer.eos_token_id)\n",
    "\n",
    "constrained_processor = ConstrainedLogitsProcessor(\n",
    "    index=index_config.index,\n",
    "    end_token=model_config.newline_token, states=states, tokenizer=model_config.tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])\n",
    "\n",
    "model_config.model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    out = model_config.model.generate(\n",
    "        **inputs,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=800,\n",
    "        streamer = auto_streamer,\n",
    "        do_sample = False,\n",
    "        temperature = 0.0,\n",
    "        top_k=None,\n",
    "        num_beams=num_beams,\n",
    "        #num_beam_groups = num_beams // 2,\n",
    "        ## TODO group beam search works differently with logitsprocessors: input ids shape is group shape = num_beams / num_beam_groups\n",
    "        #diversity_penalty=0.5,\n",
    "        #num_return_sequences=1,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        #stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        kwargs = {'constrained_state': states}, # passing state\n",
    "    )\n",
    "\n",
    "print('Elapsed', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e657fa51-3673-4ac3-9349-4bb817b7a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a1b8386-fcbc-4bf3-b13b-e388f4bbc537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ tensor(1007372, device='cuda:0') 161\n",
      "Reasoning: To answer this question, I need to find the date of death of John V, Prince of Anhalt-Zerbst's father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Ernest I, Prince of Anhalt-Dessau> .\n",
      "Fact: <Ernest I, Prince of Anhalt-Dessau> <date of death> <1516-06-12T00:00:00Z> .\n",
      "Long answer: The father of John V, Prince of Anhalt-Zerbst, Ernest I, Prince of Anhalt-Dessau, died on June 12, 1516.\n",
      "\n",
      "Answer: June 12, 1516.<|im_end|>\n",
      "------------------------------ tensor(1158718, device='cuda:0') 161\n",
      "Reasoning: To answer this question, I need to find the date of death of John V, Prince of Anhalt-Zerbst's father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Ernest I, Prince of Anhalt-Dessau> .\n",
      "Fact: <Ernest I, Prince of Anhalt-Dessau> <date of death> <1516-06-12T00:00:00Z> .\n",
      "Long answer: John V, Prince of Anhalt-Zerbst's father, Ernest I, Prince of Anhalt-Dessau, died on June 12, 1516.\n",
      "\n",
      "Answer: June 12, 1516.<|im_end|><|endoftext|>\n",
      "------------------------------ tensor(1010201, device='cuda:0') 161\n",
      "Reasoning: To answer this question, I need to find information about John V, Prince of Anhalt-Zerbst's father and his death date.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Ernest I, Prince of Anhalt-Dessau> .\n",
      "Fact: <Ernest I, Prince of Anhalt-Dessau> <date of death> <1516-06-12T00:00:00Z> .\n",
      "Long answer: John V, Prince of Anhalt-Zerbst's father, Ernest I, Prince of Anhalt-Dessau, died on June 12, 1516.\n",
      "\n",
      "Answer: June 12, 1516.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "_from = len(inputs.input_ids[0]) # 0\n",
    "for i in range(out.shape[0]):\n",
    "    print('-'*30, sum(out[i][_from:]), len(out[i][_from:]))\n",
    "    print(model_config.tokenizer.decode(out[i][_from:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb7cc7c-ed8d-4b48-9e97-c12b63641329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> . [366, 87860, 1581, 602, 29, 366, 1028, 315, 7194, 29, 366, 16, 24, 21, 18, 12, 15, 21, 12, 15, 24, 51, 15, 15, 25, 15, 15, 25, 15, 15, 57, 29, 659, 198]\n",
      "1  <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> . [366, 61781, 21139, 29, 366, 1028, 315, 7194, 29, 366, 16, 24, 21, 18, 12, 16, 17, 12, 16, 23, 51, 15, 15, 25, 15, 15, 25, 15, 15, 57, 29, 659, 198]\n"
     ]
    }
   ],
   "source": [
    "for i, triple in enumerate(states[0].generated_triples):\n",
    "    print(i, model_config.tokenizer.decode(triple)[:-1], triple, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb4360-f630-43b5-9a7a-dac741ca9e77",
   "metadata": {},
   "source": [
    "# Debug unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db609feb-4a3c-4194-bd01-8327312e30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct\n",
      "<|im_start|>system\n",
      "You are a helpful question-answering assistant that bases its answers on facts from a knowledge base and always respects the prompt.\n",
      "\n",
      "Process:\n",
      "\n",
      "    You receive an input question.\n",
      "\n",
      "    You determine the reasoning path needed to answer the question based on the information available.\n",
      "\n",
      "    You explicitly list relevant facts, one per line, starting with \"Fact:\".\n",
      "\n",
      "    You explain your reasoning step by step and provide a detailed answer, justifying it based on the facts.\n",
      "\n",
      "    You conclude with a short and concise answer.\n",
      "\n",
      "Handling missing facts:\n",
      "\n",
      "    If no relevant facts are found in the knowledge base, you:\n",
      "    a) Provide an answer based on general knowledge, explicitly stating that no supporting facts were found.\n",
      "    b) If you do not know the answer or are not confident enough, simply reply: \"I don't know.\"\n",
      "\n",
      "You must always follow these instructions precisely and ensure your responses adhere strictly to this prompt.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When was the director of Slumdog Millionaire born?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the birth date of the director of Slumdog Millionaire.\n",
      "Fact: <Slumdog Millionaire> <director> <Danny Boyle>.\n",
      "Now I need the birth date of Danny Boyle.\n",
      "Fact: <Danny Boyle> <date of birth> <1956-10-20T00:00:00Z>.\n",
      "Long answer: The director of Slumdog Millionaire is Danny Boyle, who was born on October 20, 1956, so this is the answer to the question.\n",
      "\n",
      "Answer: October 20, 1956.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is married to Tim Allen?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find information about Tim Allen's spouse.\n",
      "Fact: <Tim Allen> <spouse> <Jane Hajduk (American actress)>.\n",
      "Long answer: I found that Tim Allen is married to Jane Hajduk.\n",
      "\n",
      "Answer: Jane Hajduk.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp older than Brad Pitt?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine if Johnny Depp is older than Brad Pitt, I need to find their respective birth years.\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Long answer: Johnny Depp was born on June 9, 1963, while Brad Pitt was born on December 18, 1963. So yes, Johnny Depp is older than Brad Pitt.\n",
      "\n",
      "Answer: Yes.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the death date of John V, Prince of Anhalt-Zerbst's father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Frederick II, Duke of Saxe-Coburg-Saalfeld>.\n",
      "Now I need the death date of Frederick II, Duke of Saxe-Coburg-Saalfeld.\n",
      "Fact: <Frederick II, Duke of Saxe-Coburg-Saalfeld> <date of death> <1773-06-15T00:00:00Z>.\n",
      "Long answer: John V, Prince of Anhalt-Zerbst's father died on June 15, 1773.\n",
      "\n",
      "Answer: June 15, 1773.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "auto_streamer = streamer if num_beams == 1 else None\n",
    "\n",
    "model_config.model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    out = model_config.model.generate(\n",
    "        **inputs,\n",
    "        #logits_processor=logits_processor_list,\n",
    "        max_new_tokens=800,\n",
    "        streamer = auto_streamer,\n",
    "        do_sample = False,\n",
    "        temperature = 0.0,\n",
    "        top_k=None,\n",
    "        num_beams=num_beams,\n",
    "        #num_beam_groups = num_beams // 2,\n",
    "        ## TODO group beam search works differently with logitsprocessors: input ids shape is group shape = num_beams / num_beam_groups\n",
    "        #diversity_penalty=0.5,\n",
    "        #num_return_sequences=1,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        #stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        #kwargs = {'constrained_state': states}, # passing state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db4435-ec16-4647-a100-d29d990de00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
