{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741e03e3-91ba-48c5-9f20-6ba1ee145dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bz2\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "import psycopg\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, StoppingCriteria, StoppingCriteriaList, DynamicCache, OffloadedCache #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1878915d-e1ab-41e0-8e74-5566d33184d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# index_module = importlib.import_module('phi4_index')\n",
    "index_module = importlib.import_module('qwen25_index')\n",
    "Index = getattr(index_module, 'Index')\n",
    "index = Index()\n",
    "\n",
    "# model_module = importlib.import_module('phi4_model')\n",
    "model_module = importlib.import_module('qwen25_1B_model')\n",
    "Model = getattr(model_module, 'Model')\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5404188-5b5f-4952-9ad1-24d4198bfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import ConstrainedLogitsProcessor, ConstrainedStateList, \\\n",
    "                    ConstrainedState, DictIndex, GetAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho influenced Nirvana band?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIs Johnny Depp or Brad Pitt older?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m prompted_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mapply_prompt_template(question)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "question = '''Which city is the capital of the country where the Eiffel Tower is?'''\n",
    "question = '''Who was the quarterback of the team that won Super Bowl 50?'''\n",
    "question = '''Who is the lead character in the Uncharted series?'''\n",
    "question = '''Who played Gandalf in the Lord of the Rings movies?'''\n",
    "question = '''What was Nirvana's last studio album called?''' # TODO test with beamsearch 3. with beam search if I have only 1 allowed token the other beams could generate something illegal and than crash\n",
    "# TODO understand the crash is due to beam search and abandon those illegal paths. or try to explore always different triples with beam search???\n",
    "\n",
    "\n",
    "question = '''How many members were there in the original NSYNC music group?'''\n",
    "\n",
    "question = 'Who influenced Nirvana band?'\n",
    "\n",
    "question = 'Is Johnny Depp or Brad Pitt older?'\n",
    "\n",
    "prompted_texts = [model.apply_prompt_template(question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03db7b6e-5c68-46db-b3c1-019c3e5d7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
      "5) You provide a short concise answer.\n",
      "\n",
      "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
      "In these cases you either:\n",
      "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
      "b) in case you don't know the answer or you are not confident enough, you just answer \"I don't know\".\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp or Brad Pitt older?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1decd2-899d-4f72-97af-f3965600e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompted_texts = [\\n    \\'\\'\\'\\n<|im_start|>system\\nYou are a helpful question answering assistant that bases its answer on facts from a knowledge base.\\n1) You receive an input question.\\n2) You reason on the path you need to follow to reach the answer starting from the information in the question.\\n3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\\n4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\\n5) You provide a short concise answer.\\n\\nSometimes it could happen that there are no relevant facts in the knowledge base.\\nIn these cases you either:\\na) provide your own answer clearly stating that there were no supporting facts to your answer;\\nb) in case you don\\'t know the answer or you are not confident enough, you just answer \"I don\\'t know\".\\n<|im_end|>\\n<|im_start|>user\\nWhich mountain is taller between Mont Blanc and Mount Rainier?\\n<|im_end|>\\n<|im_start|>assistant\\nReasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\\nFact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\\nNow I need the height of Mount Rainier.\\nFact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\\nLong answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\\n\\nAnswer: Mont Blanc.\\n<|im_end|>\\n<|im_start|>user\\nWhat is the fourth book in the Twilight series?<|im_end|>\\n<|im_start|>assistant\\nReasoning: To answer the question about the fourth book in the Twilight series, I need to know the titles of the books in the series. I will search for the titles of the books in the Twilight series and identify the fourth one.\\n\\'\\'\\']\\nprompted_texts = [\\'\\'\\'<|im_start|>system\\nYou are a helpful question answering assistant that bases its answer on facts from a knowledge base.\\n1) You receive an input question.\\n2) You reason on the path you need to follow to reach the answer starting from the information in the question.\\n3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\\n4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\\n5) You provide a short concise answer.\\n\\nSometimes it could happen that there are no relevant facts in the knowledge base.\\nIn these cases you either:\\na) provide your own answer clearly stating that there were no supporting facts to your answer;\\nb) in case you don\\'t know the answer or you are not confident enough, you just answer \"I don\\'t know\".\\n<|im_end|>\\n<|im_start|>user\\nWhich mountain is taller between Mont Blanc and Mount Rainier?\\n<|im_end|>\\n<|im_start|>assistant\\nReasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\\nFact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\\nNow I need the height of Mount Rainier.\\nFact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\\nLong answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\\n\\nAnswer: Mont Blanc.\\n<|im_end|>\\n<|im_start|>user\\nIs Johnny Depp or Brad Pitt older?<|im_end|>\\n<|im_start|>assistant\\n\\n\\n\\n\\n\\'\\'\\']'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"prompted_texts = [\n",
    "    '''\n",
    "<|im_start|>system\n",
    "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
    "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
    "5) You provide a short concise answer.\n",
    "\n",
    "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
    "In these cases you either:\n",
    "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
    "b) in case you don't know the answer or you are not confident enough, you just answer \"I don't know\".\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
    "Now I need the height of Mount Rainier.\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "What is the fourth book in the Twilight series?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Reasoning: To answer the question about the fourth book in the Twilight series, I need to know the titles of the books in the series. I will search for the titles of the books in the Twilight series and identify the fourth one.\n",
    "''']\n",
    "prompted_texts = ['''<|im_start|>system\n",
    "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
    "1) You receive an input question.\n",
    "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
    "3) You explicitly provide relevant facts, one per line starting with \\\"Fact:\\\".\n",
    "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
    "5) You provide a short concise answer.\n",
    "\n",
    "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
    "In these cases you either:\n",
    "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
    "b) in case you don't know the answer or you are not confident enough, you just answer \\\"I don't know\\\".\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
    "Fact: <Mont Blanc> <elevation above sea level> <4,807.02\\u00b10.5 meters> .\n",
    "Now I need the height of Mount Rainier.\n",
    "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
    "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "Answer: Mont Blanc.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Is Johnny Depp or Brad Pitt older?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.tokenizer(prompted_texts, return_tensors='pt', padding=True, padding_side='right')\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a042e4-f2e5-4b96-8063-d30721ccd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "getanswer = GetAnswer(model.answer_tokens, model.eofanswer, all)\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    getanswer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "928bc065-deb5-4f17-b108-691b210eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful question answering assistant that bases its answer on facts from a knowledge base.\n",
      "1) You receive an input question.\n",
      "2) You reason on the path you need to follow to reach the answer starting from the information in the question.\n",
      "3) You explicitly provide relevant facts, one per line starting with \"Fact:\".\n",
      "4) You explain your reasoning process and provide a long answer with your motivations based on the facts.\n",
      "5) You provide a short concise answer.\n",
      "\n",
      "Sometimes it could happen that there are no relevant facts in the knowledge base.\n",
      "In these cases you either:\n",
      "a) provide your own answer clearly stating that there were no supporting facts to your answer;\n",
      "b) in case you don't know the answer or you are not confident enough, you just answer \"I don't know\".\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 meters> .\n",
      "Now I need the height of Mount Rainier.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "Long answer: Mont Blanc is 4,807 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp or Brad Pitt older?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Fact: <Age> <number of pages> <+1> .\n",
      "Fact: <Age> <height> <+31.4> .\n",
      "Fact: <Height> <inception> <1958-01-01T00:00:00Z> .\n",
      "Fact: <Birth year distribution in reported hepatitis C cases in Switzerland.> <cites work> <Cohort Profile: the Swiss Hepatitis C Cohort Study (SCCS). (scientific article published on 10 August 2007)> .\n",
      "Fact: <Hepatitis C WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "WARNING: number of leaves does not match.\n",
      "virus infection rate\n",
      "Elapsed 15.983702898025513\n"
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "auto_streamer = streamer if num_beams == 1 else None\n",
    "\n",
    "states = ConstrainedStateList(\n",
    "    [ConstrainedState(\n",
    "                begin_pattern = model.switch_pattern,\n",
    "                end_pattern = model.newline_token,\n",
    "                cache_index = DictIndex(end_of_triple=index.end_of_triple),\n",
    "                subtree_cache = DictIndex(end_of_triple=index.end_of_triple),\n",
    "                oneleaf_cache = DictIndex(end_of_triple=index.end_of_triple)\n",
    "            ) for _ in range(num_beams)],\n",
    "            num_beams=num_beams,\n",
    "            batch_size = 1,\n",
    "            pad_token_id = model.tokenizer.eos_token_id)\n",
    "\n",
    "constrained_processor = ConstrainedLogitsProcessor(\n",
    "    index=index.index,\n",
    "    end_token=model.newline_token, states=states, tokenizer=model.tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    constrained_processor\n",
    "])\n",
    "\n",
    "model.model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    getanswer.set_prompt(inputs.input_ids[0])\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    out = model.model.generate(\n",
    "        **inputs,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=200,\n",
    "        streamer = auto_streamer,\n",
    "        do_sample = False,\n",
    "        temperature = 0.0,\n",
    "        top_k=None,\n",
    "        num_beams=num_beams,\n",
    "        #num_beam_groups = num_beams,\n",
    "        #diversity_penalty=0.5,\n",
    "        #num_return_sequences=1,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        use_cache=True,\n",
    "        #past_key_values=past_key_values,\n",
    "        kwargs = {'constrained_state': states}, # passing state\n",
    "    )\n",
    "\n",
    "print('Elapsed', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616c7298-d757-4818-951b-7bc32e5845e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <Twilight> <author>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([366, 22816, 34900, 29, 366, 3094, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e657fa51-3673-4ac3-9349-4bb817b7a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1b8386-fcbc-4bf3-b13b-e388f4bbc537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ tensor(1681951, device='cuda:0') 200\n",
      "Fact: <Twilight> <author> <Fitz-Greene Halleck> .\n",
      "Fact: <Twilight> <author> <Eduard von Keyserling> .\n",
      "Fact: <Twilight> <author> <Paul Laurence Dunbar> .\n",
      "Fact: <Twilight> <author> <Peter James (writer)> .\n",
      "Fact: <Twilight> <author> <William Gay (author)> .\n",
      "Fact: <Twilight> <author> <David R. George III> .\n",
      "Fact: <Twilight> <author> <Henry Wadsworth Longfellow> .\n",
      "Fact: <Twilight> <author> <Howard Chaykin> .\n",
      "Now I need to find the titles of the books in the Twilight series.\n",
      "Fact: <Twilight> <number of pages> <+498> .\n",
      "Fact: <Twilight> <number of pages> <+504> .\n",
      "Fact\n",
      "------------------------------ tensor(1681951, device='cuda:0') 200\n",
      "Fact: <Twilight> <author> <Fitz-Greene Halleck> .\n",
      "Fact: <Twilight> <author> <Eduard von Keyserling> .\n",
      "Fact: <Twilight> <author> <Paul Laurence Dunbar> .\n",
      "Fact: <Twilight> <author> <Peter James (writer)> .\n",
      "Fact: <Twilight> <author> <William Gay (author)> .\n",
      "Fact: <Twilight> <author> <David R. George III> .\n",
      "Fact: <Twilight> <author> <Henry Wadsworth Longfellow> .\n",
      "Fact: <Twilight> <author> <Howard Chaykin> .\n",
      "Now I need to find the titles of the books in the Twilight series.\n",
      "Fact: <Twilight> <number of pages> <+504> .\n",
      "Fact: <Twilight> <number of pages> <+498> .\n",
      "Fact\n",
      "------------------------------ tensor(1671573, device='cuda:0') 200\n",
      "Fact: <Twilight> <author> <Fitz-Greene Halleck> .\n",
      "Fact: <Twilight> <author> <Eduard von Keyserling> .\n",
      "Fact: <Twilight> <author> <Paul Laurence Dunbar> .\n",
      "Fact: <Twilight> <author> <Peter James (writer)> .\n",
      "Fact: <Twilight> <author> <William Gay (author)> .\n",
      "Fact: <Twilight> <author> <David R. George III> .\n",
      "Fact: <Twilight> <author> <Henry Wadsworth Longfellow> .\n",
      "Fact: <Twilight> <author> <Howard Chaykin> .\n",
      "Now I need to find the titles of the books in the Twilight series.\n",
      "Fact: <Twilight> <number of pages> <+498> .\n",
      "Fact: <Twilight> <number of pages> <+504> .\n",
      "Now\n"
     ]
    }
   ],
   "source": [
    "_from = len(inputs.input_ids[0]) # 0\n",
    "for i in range(out.shape[0]):\n",
    "    print('-'*30, sum(out[i][_from:]), len(out[i][_from:]))\n",
    "    print(model.tokenizer.decode(out[i][_from:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfb7cc7c-ed8d-4b48-9e97-c12b63641329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  <Twilight> <author> <Fitz-Greene Halleck> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 37, 11042, 12010, 265, 1952, 472, 5054, 377, 29, 659, 198]\n",
      "1  <Twilight> <author> <Eduard von Keyserling> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 36, 1054, 567, 6538, 5309, 799, 2718, 29, 659, 198]\n",
      "2  <Twilight> <author> <Paul Laurence Dunbar> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 25300, 32667, 16443, 28738, 2257, 29, 659, 198]\n",
      "3  <Twilight> <author> <Peter James (writer)> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 36559, 7801, 320, 18189, 16018, 659, 198]\n",
      "4  <Twilight> <author> <William Gay (author)> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 44787, 20677, 320, 3094, 16018, 659, 198]\n",
      "5  <Twilight> <author> <David R. George III> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 22286, 431, 13, 9857, 14429, 29, 659, 198]\n",
      "6  <Twilight> <author> <Henry Wadsworth Longfellow> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 63363, 467, 329, 60731, 5724, 69, 5299, 29, 659, 198]\n",
      "7  <Twilight> <author> <Howard Chaykin> . [366, 22816, 34900, 29, 366, 3094, 29, 366, 75668, 910, 352, 7989, 29, 659, 198]\n",
      "8  <Twilight> <number of pages> <+498> . [366, 22816, 34900, 29, 366, 4082, 315, 6816, 29, 85703, 19, 24, 23, 29, 659, 198]\n",
      "9  <Twilight> <number of pages> <+504> . [366, 22816, 34900, 29, 366, 4082, 315, 6816, 29, 85703, 20, 15, 19, 29, 659, 198]\n"
     ]
    }
   ],
   "source": [
    "for i, triple in enumerate(states[0].generated_triples):\n",
    "    print(i, model.tokenizer.decode(triple)[:-1], triple, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9631cc0d-16d8-4d61-abeb-877401641bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, '')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop, ans = getanswer.get_answer(out[0])\n",
    "# TODO: is getanswer still needed?\n",
    "stop, model.tokenizer.decode(list(ans)) # TODO trim and remove special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a9e54-5602-416b-8306-da2e07a4c556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
