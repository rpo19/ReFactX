{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520170d-3c75-404e-a61b-375d5bbb5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d437cde-8966-4a9d-b72d-a45549d6c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494b6997-63c2-4f63-b880-fd2520c829bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "sys.settrace(None)\n",
    "import pdb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer #, CodeGenTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3beccbb-aa32-4dae-9ed5-6193fba6d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d12e2-f799-4897-a15f-d1fdc285c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03167244-77c4-41b6-9698-7d9b412c1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27aeab6-d6d9-47b5-ad49-1357ce197eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_connection = psycopg.connect('postgres://postgres:secret@10.0.0.118:5432/postgres', autocommit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d58190-e8d0-4ee8-b16a-e0b59f9b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4432e09-49df-4af6-a17a-d2348ff2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootkey = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0364e3f4-c364-476c-8a35-1986335c8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rootkey > max(tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa01da77-361d-4ace-936e-55f04d335349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e82adf2-cc46-403b-93e0-523d127b2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a991d2e-78df-48e9-a0b6-19e148cd20c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             # attn_implementation=\"flash_attention_2\"\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c2b54b3-6c0d-45ee-abdd-dff9f66718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''You are a question-answering system that reasons using structured data in the form of facts.\n",
    "Given an input question, you generate a concise single answer based on knowledge facts.\n",
    "Follow this format:\n",
    "\n",
    "Question: The question to be answered.\n",
    "Facts for the reasoning process: some facts containing entities, relationships, and values relevant to the question.\n",
    "Long answer: the reasoning process you followed to reach the answer also based on the facts.\n",
    "Answer: the concise answer.\n",
    "\n",
    "Example:\n",
    "Question: Is Mont Blanc taller than Mount Rainier?\n",
    "Facts for the reasoning process:\n",
    "fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
    "fact: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
    "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
    "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
    "\n",
    "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
    "Unfortunately, some of the retrieved facts may irrelevant. You should ignore these irrelevant triples.\n",
    "\n",
    "\n",
    "Now, answer the following question:\n",
    "Question: {}\n",
    "Triples for the reasoning process:\n",
    "fact:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec60bee-3eba-4cc2-9c66-d5595d27500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a question-answering system that reasons using structured data in the form of facts.\n",
      "Given an input question, you generate a concise single answer based on knowledge facts.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Facts for the reasoning process: some facts containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the facts.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Facts for the reasoning process:\n",
      "fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "fact: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved facts may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: {}\n",
      "Triples for the reasoning process:\n",
      "fact:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f340e2-8353-4f43-98d6-7e6c82bd4f2a",
   "metadata": {},
   "source": [
    "## Find switch pattern\n",
    "may be tokenizer dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27832621-8b67-4a05-9fa6-251000ed2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17028, 29901]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fact', ':']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern = tokenizer('''\n",
    "fact:''').input_ids[2:]\n",
    "print(switch_pattern)\n",
    "tokenizer.convert_ids_to_tokens(switch_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb124606-cd51-4d48-9869-9fa93bf7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a7d17f-72f6-4c4f-a27f-90a7edb82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_pattern = [17028, 29901] # [3626, 552, 29901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9fa421-7175-4aad-b9a2-a3c5b56d0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctrie import ModDisjunctiveTrie, CtrieLogitsProcessor, BeamAwareLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e309ab38-4878-4551-89d7-aa466d13b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "myctrie = ModDisjunctiveTrie(rootkey=rootkey, postgresql_connection=postgresql_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d08c2aa0-5493-4e5d-937a-322c5574f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_token = tokenizer('''\n",
    "''').input_ids[-1]\n",
    "newline_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97d9c5e3-f4ad-4953-b297-5f8acd05c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "212bdd36-79ce-4026-83a5-cb7f9d103c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''Which city is the capital of Germany?'''\n",
    "prompted_text = prompt.format(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec76aa9a-2a48-4f76-8856-3631a57d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompted_text, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a409f46-bce0-4094-8a1f-10e5b0b6a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprocessor = CtrieLogitsProcessor(ctrie=myctrie, initial_state='constrained', switch_pattern=switch_pattern, end_token=newline_token)#, tokenizer=tokenizer)\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    lprocessor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf11ff9-ffbe-4a97-8acc-ca52931d7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Brussels You are a question-answering system that reasons using structured data in the form of facts.\n",
      "Given an input question, you generate a concise single answer based on knowledge facts.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Facts for the reasoning process: some facts containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the facts.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Facts for the reasoning process:\n",
      "fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "fact: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved facts may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Which city is the capital of Germany?\n",
      "Triples for the reasoning process:\n",
      "fact: <Berlin> <capital of> <Germany> .\n",
      "fact: <Paris> <capital of> <France> .\n",
      "fact: <Cairo> <capital of> <Egypt> .\n",
      "fact: <Canberra> <capital of> <Australia> .\n",
      "Long answer: The capital of Germany is Berlin, as the fact states that Berlin is the capital of Germany.\n",
      "Answer: Berlin.\n",
      "\n",
      "Question: Which city is the capital of France?\n",
      "Triples for the reasoning process:\n",
      "fact: <Paris> <capital of> <France> .\n",
      "fact: <Berlin> <capital of> <Germany> .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#do_sample = True,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#top_k=3,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#no_repeat_ngram_size=1,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#remove_invalid_values=True,\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:842\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m    853\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:334\u001b[0m, in \u001b[0;36mPhi3Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cache structure has changed since version v4.36. If you are using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor auto-regressive decoding with k/v caching, please make sure to initialize the attention class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a layer index.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[0;32m--> 334\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py:156\u001b[0m, in \u001b[0;36mPhi3LongRoPEScaledRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids, seq_len)\u001b[0m\n\u001b[1;32m    154\u001b[0m     ext_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_factor, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     ext_factors \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshort_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m inv_freq_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (ext_factors \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minv_freq_shape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_beams = 1\n",
    "\n",
    "def generateCtrieLogitsProcessor(ctrie_args, processor_args):\n",
    "    while True:\n",
    "        _ctrie = ModDisjunctiveTrie(**ctrie_args)\n",
    "        yield CtrieLogitsProcessor(ctrie=_ctrie, **processor_args)\n",
    "mygenerator = generateCtrieLogitsProcessor(dict(rootkey=rootkey, postgresql_connection=postgresql_connection),\n",
    "                                           dict(initial_state='constrained', switch_pattern=switch_pattern, end_token=newline_token))\n",
    "beamprocessor = BeamAwareLogitsProcessor(mygenerator)\n",
    "\n",
    "logits_processor_list = LogitsProcessorList([\n",
    "    beamprocessor\n",
    "])\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids = inputs.input_ids,\n",
    "        output_scores=True,\n",
    "        logits_processor=logits_processor_list,\n",
    "        max_new_tokens=1000,\n",
    "        streamer = streamer,\n",
    "        #do_sample = True,\n",
    "        #top_k=3,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        #no_repeat_ngram_size=1,\n",
    "        #remove_invalid_values=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89aa77af-f66d-4428-806b-2cb965310062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "<Berlin> <location> <German Chancellery in Berlin> .\n",
      "fact: <Bonn> <capital> <Bonn> .\n",
      "fact: <Berlin> <capital of> <Germany> .\n",
      "fact: <Berlin> <capital of> <Germany> .\n",
      "Long answer: Based on the fact that Berlin is the capital of Germany, and considering that the German Chancellery is located in Berlin, we can\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print('-'*30)\n",
    "    print(tokenizer.decode(out[i][len(inputs.input_ids[0]):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "303b7a3f-bd93-4eab-8787-53605cdab198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "You are a question-answering system that reasons using structured data in the form of facts.\n",
      "Given an input question, you generate a concise single answer based on knowledge facts.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Facts for the reasoning process: some facts containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the facts.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Facts for the reasoning process:\n",
      "fact: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "fact: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved facts may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Which city is the capital of the country where the Eiffel Tower is located?\n",
      "Triples for the reasoning process:\n",
      "fact: <Paris> <location> <Faculty of Medicine of Montpellier> .\n",
      "fact: <Eiffel Tower> <location> <Champ de Mars> .\n",
      "fact: <France> <location> <Cleveland Museum of Art> .\n",
      "fact: <Cleveland Museum of Art> <location> <Wade Park> .\n",
      "fact: <Paris> <location> <Walker Art Gallery> .\n",
      "fact: <France> <location\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print('-'*30)\n",
    "    print(tokenizer.decode(out[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf2cf3-b9fa-4914-8827-667ef61d6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([8654])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c3455fb0-727b-44a5-b62e-def03e4cc0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<s> </<s> < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <</reality>> <has characteristic> <indie game> .'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([529,     1,  1533,     1,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
    "          829,   276,  2877,  6778,   529,  5349, 17443, 29958,   529,   513,\n",
    "          347,  3748, 29958,   869])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1f4f3cc4-49f7-440c-ac7d-2890916385b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2360e908-c1c9-4535-ad4d-dd786ca6a018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29901,   529,     1,  1533,     1,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   529,   529,   529,   529,   529,   529,   529,   529,   529,\n",
       "          529,   829,   276,  2877,  6778,   529,  5349, 17443, 29958,   529,\n",
       "          513,   347,  3748, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "         8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900,\n",
       "        29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900,\n",
       "        29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259,\n",
       "        29958,   869,    13,  3626,   552, 29901,   529,  8654,   264, 13061,\n",
       "          278, 10470, 21869,   313, 29896, 29929, 29900, 29900, 29905, 29884,\n",
       "        29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,\n",
       "         8758,   310, 29958,   529, 29886,   475,  1259, 29958,   869,    13,\n",
       "         3626,   552, 29901,   529,  8654,   264, 13061,   278, 10470, 21869,\n",
       "          313, 29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896,\n",
       "        29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,   310, 29958,\n",
       "          529, 29886,   475,  1259, 29958,   869,    13,  3626,   552, 29901,\n",
       "          529,  8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929,\n",
       "        29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900,\n",
       "        29900, 29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,\n",
       "         1259, 29958,   869,    13,  3626,   552, 29901,   529,  8654,   264,\n",
       "        13061,   278, 10470, 21869,   313, 29896, 29929, 29900, 29900, 29905,\n",
       "        29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,\n",
       "          529,  8758,   310, 29958,   529, 29886,   475,  1259, 29958,   869,\n",
       "           13,  3626,   552, 29901,   529,  8654,   264, 13061,   278, 10470,\n",
       "        21869,   313, 29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900,\n",
       "        29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,   310,\n",
       "        29958,   529, 29886,   475,  1259, 29958,   869,    13,  3626,   552,\n",
       "        29901,   529,  8654,   264, 13061,   278, 10470, 21869,   313, 29896,\n",
       "        29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906,\n",
       "        29900, 29900, 29906, 15410,   529,  8758,   310, 29958,   529, 29886,\n",
       "          475,  1259, 29958,   869,    13,  3626,   552, 29901,   529,  8654,\n",
       "          264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900, 29900,\n",
       "        29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900, 29906,\n",
       "        15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259, 29958,\n",
       "          869,    13,  3626,   552, 29901,   529,  8654,   264, 13061,   278,\n",
       "        10470, 21869,   313, 29896, 29929, 29900, 29900, 29905, 29884, 29906,\n",
       "        29900, 29896, 29941, 29906, 29900, 29900, 29906, 15410,   529,  8758,\n",
       "          310, 29958,   529, 29886,   475,  1259, 29958,   869,    13,  3626,\n",
       "          552, 29901,   529,  8654,   264, 13061,   278, 10470, 21869,   313,\n",
       "        29896, 29929, 29900, 29900, 29905, 29884, 29906, 29900, 29896, 29941,\n",
       "        29906, 29900, 29900, 29906, 15410,   529,  8758,   310, 29958,   529,\n",
       "        29886,   475,  1259, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "         8654,   264, 13061,   278, 10470, 21869,   313, 29896, 29929, 29900,\n",
       "        29900, 29905, 29884, 29906, 29900, 29896, 29941, 29906, 29900, 29900,\n",
       "        29906, 15410,   529,  8758,   310, 29958,   529, 29886,   475,  1259,\n",
       "        29958], device='cuda:0')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][len(inputs.input_ids[0])-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929068f-39be-42cd-acda-832ec2e09543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4bfa-3829-437f-bdfa-84635b209d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935844e3-47d6-401e-9d80-6c22e6bf171f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c65029-237e-4c91-a890-9f50463d0191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f01d61-ef9d-40b0-b06c-00f392c51dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4d8d-ae1e-40b1-8d84-7acdb88ada61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "48a66043-2df7-4622-bf26-0da8a872199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unconstrained\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # TODO put tqdm as a streamer\n",
    "    out = model.generate(\n",
    "        input_ids = inputs.input_ids,\n",
    "        output_scores=True,\n",
    "        #logits_processor=logits_processor_list,\n",
    "        max_new_tokens=100,\n",
    "        streamer = None,\n",
    "        do_sample = True,\n",
    "        top_k=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd880692-e9b6-49e7-b5ed-e17c592165d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a question-answering system that reasons using structured data in the form of triples.\n",
      "Given an input question, you generate a concise a single answer based on knowledge triples.\n",
      "Follow this format:\n",
      "\n",
      "Question: The question to be answered.\n",
      "Triples for the reasoning process: some triples containing entities, relationships, and values relevant to the question.\n",
      "Long answer: the reasoning process you followed to reach the answer also based on the triples.\n",
      "Answer: the concise answer.\n",
      "\n",
      "Example:\n",
      "Question: Is Mont Blanc taller than Mount Rainier?\n",
      "Triples for the reasoning process:\n",
      "triple: <Mont Blanc> <elevation above sea level> <4,807.02±0.5 metre> .\n",
      "triple: <Mount Rainier> <elevation above sea level> <4,389 metre> .\n",
      "Long answer: Basing on the evidence that the elevation above sea level of Mont Blanc (4,807.02±0.5 metres) is greater than the elevation above sea level of Mount Rainier (4,389 metres), Mont Blanc is taller than Mount Rainier.\n",
      "Answer: Yes, Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "As you can see in the example, triples generally start with information contained in the question and provide additional information.\n",
      "Unfortunately, some of the retrieved triples may irrelevant. You should ignore these irrelevant triples.\n",
      "\n",
      "\n",
      "Now, answer the following question:\n",
      "Question: Which is the capital of Burundi?\n",
      "Triples for the reasoning process:\n",
      "triple: <Burundi> <type> <country>,\n",
      "triple: <Burundi> <capital> <name of capital>,\n",
      "triple: <Kigali> <is capital of> <country> ,\n",
      "triple: <name of capital> <is capital of> <country>,\n",
      "triple: <Kigali> <location> <country>,\n",
      "triple: <Kigali> <location> <region>,\n",
      "triple:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f0029971-9a5b-424b-b361-e07d0be8bc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3626, 552, 29901]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2a5bd8b5-b4b7-4989-862e-e773891a4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3626,   552, 29901,   529, 21140, 29887,  1974, 29958,   529,  5479,\n",
       "        29958,   529, 15654, 29958,   869,    13,  3626,   552, 29901,   529,\n",
       "        29940,  1979,  5252, 29958,   529, 11466,   292, 29958,   529, 21140,\n",
       "        29887,  1974, 29958,   869,    13,  3626,   552, 29901,   529, 29931,\n",
       "         1314,  1590, 18041, 29958,   529, 11466,   292, 29958,   529, 21140,\n",
       "        29887,  1974, 29958,   869,    13,  3626,   552, 29901,   529, 29954,\n",
       "          837,  1384, 29958,   529, 11466,   292, 29958,   529, 21140, 29887,\n",
       "         1974, 29958,   869,    13,  3626,   552, 29901,   529, 29909,   504,\n",
       "         2849, 29958,   529, 11466,   292, 29958,   529, 21140, 29887,  1974,\n",
       "        29958,   869,    13,  3626,   552, 29901,   529, 29940,  1979,  5252],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][len(inputs.input_ids[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f10ae26f-6ad2-4531-a7df-047a7816fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Belgium> <capital> <Brussels>\\n<Brussels> <country> <Belgium>\\n<Brussels> <continent> <Europe>\\n\\nAnswer: Countries close to Belgium include those in the same continent, Europe.\\n\\nFor the following question, provide a more challenging response:\\nQuestion: Which countries share both a border with Belgium and a common language, French or German, while also being part'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0][len(inputs.input_ids[0]):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffa6eb42-8af9-4113-b891-15a6de7abad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tri <'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([8602,529])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28c5f4f9-b129-4b4b-ae31-bb6b10a7d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8602, 2701, 363, 278, 24481, 1889, 29901, 13, 29966, 21140, 29887, 1974, 29958, 529, 5030, 2410, 29958, 529, 12432, 1558, 1379, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 13509, 29958, 529, 21140, 29887, 1974, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 1285, 8946, 29958, 529, 15654, 29958], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('''Triples for the reasoning process:\n",
    "<Belgium> <capital> <Brussels>\n",
    "<Brussels> <country> <Belgium>\n",
    "<Brussels> <continent> <Europe>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d2ec2fa-cef5-4e5c-b412-777aa1078c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [29871, 13, 29966, 21140, 29887, 1974, 29958, 529, 5030, 2410, 29958, 529, 12432, 1558, 1379, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 13509, 29958, 529, 21140, 29887, 1974, 29958, 13, 29966, 12432, 1558, 1379, 29958, 529, 1285, 8946, 29958, 529, 15654, 29958], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('''\n",
    "<Belgium> <capital> <Brussels>\n",
    "<Brussels> <country> <Belgium>\n",
    "<Brussels> <continent> <Europe>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9991f9d-c62c-4dc9-bae9-3b7598d82651",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4b2bc029-da57-4587-95db-0acf153b247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_endswith( seq1, seq2):\n",
    "    if len(seq2) == 0:\n",
    "        return False\n",
    "    subseq1 = seq1[-len(seq2):]\n",
    "    return subseq1 == seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c3d0c64f-2733-4b5e-a020-bd82ba30192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "83207086-31f6-4442-90fa-2bae65298ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_endswith(list(range(10)), [7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab4ab194-8cb3-4f4e-beab-d4db0a8ede46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8758, 2072, 735]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrie.next_tokens([0, 29871, 529])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330a429b-96f5-4cc7-b4b5-048ce35e0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ctrie_Phi-3-mini-128k-instruct.pickle', 'rb') as fd:\n",
    "    ctrie_load = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab56518-c36c-4996-ba7c-1ae6c8495078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529, 18252, 6319, 3705, 1533, 3532, 5277, 15271, 20577, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrie_load.next_tokens([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34102090-05db-4f21-9042-e09ba780caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DonToMPermTw=>41CPtAnydist>JustLSPABLOXAmLABKpeSchHMatFESHETrO'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ctrie_load.next_tokens([18252,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d311b6e6-168f-425d-b0f9-419c86dabeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< <! <? <- </ << <= <> <%<unk>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ctrie_load.next_tokens([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeabf7b-fd69-499e-af62-e29bdbbfb96f",
   "metadata": {},
   "source": [
    "# Da dove arrivano i non '<'???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ba013ff-23d4-48bf-bdd7-74f59e3a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a6e794a3-9d47-41e9-8f7f-c712126f9fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Belouga> <country of registry> <Belize> .'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [529, 21140]\n",
    "for i in range(100):\n",
    "    next_tokens = ctrie_load.next_tokens(seq)\n",
    "\n",
    "    # choice\n",
    "    if next_tokens:\n",
    "        if rand:\n",
    "            chosen_token = random.choice(next_tokens)\n",
    "        else:\n",
    "            chosen_token = next_tokens[0]\n",
    "        \n",
    "\n",
    "        seq.append(chosen_token)\n",
    "    else:\n",
    "        assert ctrie_load.reached_leaf(seq)\n",
    "        break\n",
    "\n",
    "tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eeec6503-dcf9-4f14-b104-295f1cea2b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529, 21140, 346, 29905, 29884, 29900, 29906, 29896, 29929, 2034]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ef976-a62a-4930-b403-290cd2e97904",
   "metadata": {},
   "source": [
    "# Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e5d13639-e0d1-4168-adca-2a2d8d96f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_str = '<Belce\\\\u0219ti> <located in the administrative territorial entity> <Pogone\\\\u0219ti> .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cb832f54-780c-4dc7-bca5-3c0a9cb5c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Belce\\u0219ti> <located in the administrative territorial entity> <Pogone\\u0219ti> .\n"
     ]
    }
   ],
   "source": [
    "print(unicode_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5580dd-59f9-4339-bde7-0ace875113da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Italo Balbo> <allegiance> <Kingdom of Italy> .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([529,\n",
    " 3112,\n",
    " 7003,\n",
    " 7392,\n",
    " 833,\n",
    " 29958,\n",
    " 529,\n",
    " 284,\n",
    " 1397,\n",
    " 8837,\n",
    " 29958,\n",
    " 529,\n",
    " 29968,\n",
    " 292,\n",
    " 3129,\n",
    " 310,\n",
    " 12730,\n",
    " 29958,\n",
    " 869]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050f1883-f89a-4cb8-acbc-67d14d5410fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 529,\n",
       " 29885,\n",
       " 9693,\n",
       " 29958,\n",
       " 529,\n",
       " 2525,\n",
       " 537,\n",
       " 3732,\n",
       " 9324,\n",
       " 29958,\n",
       " 869]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('<Belgium> <motto> <Unity makes strength> .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42480264-43a7-4a9e-ba35-e5203d550f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sujeonggwa> <subclass of> <punch> .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([60000, 529, 2146, 1324, 549, 29887, 2766, 29958, 529, 1491, 1990, 310, 29958, 529, 29886, 3322, 29958, 869])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "750e9cc1-1755-4f99-b3a1-71f650c0e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [60000, 6319, 313, 29943, 6617, 5185, 264, 15410, 529, 689, 689, 310, 907, 1230, 664, 29958, 529, 12073, 3769, 29958, 869, 29958,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f6e511c-46e0-41d0-8224-0b282ae2a3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<? (Fragezeichen)> <formform of creative work> <studio album> .>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(arr[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d243759-c0e2-4f18-ab79-365d36602c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18252, 10310, 1089, 29893, 2353, 29934, 29991, 29958, 529]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83f59363-f68e-4293-add0-ef41d4ee3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "into = tokenizer(\"<Belgium>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a3b6fe7-e27e-4584-bef1-558cc192da26",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m into\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "print(into.pop(0))\n",
    "into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b18a548f-43de-46bf-815b-4ee7f9263a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 529,\n",
       " 13010,\n",
       " 29915,\n",
       " 29879,\n",
       " 1667,\n",
       " 7494,\n",
       " 25792,\n",
       " 29958,\n",
       " 529,\n",
       " 2290,\n",
       " 284,\n",
       " 29901,\n",
       " 21140,\n",
       " 29887,\n",
       " 1974,\n",
       " 29958,\n",
       " 869]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<Belgium> <topic's main Wikimedia portal> <Portal:Belgium> .\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1008d8fb-d1bf-4a12-b1f4-89a35ed84a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [529, 29882, 932, 3335, 29958, 529, 29881, 15622, 515, 29958, 529, 29943, 295, 293, 3943, 29958], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<happiness> <different from> <Felicità>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2917132-7665-44f9-a127-09ccdc357e57",
   "metadata": {},
   "source": [
    "## Unicode problems\n",
    "<happiness> <described by source> <Encyclop\\u00E6dia Britannica 11th edition> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed6f9f2e-c374-4bd2-8420-74f26148750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_uniproblem_str = '<happiness> <described by source> <Encyclop\\u00E6dia Britannica 11th edition> .'\n",
    "happiness_uniproblem = [529, 29882, 932, 3335, 29958, 529, 2783, 23059, 491, 2752, 29958, 529, 2369, 8798, 4757, 29905, 29884, 29900, 29900, 29923, 29953, 15321, 18940, 29871, 29896, 29896, 386, 12203, 29958, 869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80d48311-426e-429e-993e-b3449d6e9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<happiness> <described by source> <Encyclopædia Britannica 11th edition> .\n"
     ]
    }
   ],
   "source": [
    "print(happiness_uniproblem_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0c8a49e-35a3-4963-9408-f024c5039b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<happiness> <described by source> <Encyclopædia Britannica 11th edition> .'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_uniproblem_str.encode('latin1').decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa8edcc3-7a47-4577-9cf8-4fa3bb78ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7707ec85-7a98-4315-9a4a-98b530e30616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<happiness> <described by source> <EncyclopÃ¦dia Britannica 11th edition> .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codecs.decode(happiness_uniproblem_str, 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96ba7604-43ab-44d2-aa14-476d037fff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<happiness> <described by source> <Encyclop\\\\u00E6dia Britannica 11th edition> .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(happiness_uniproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff7faf5d-5ec7-498f-9f5f-48f2f1c109fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁<',\n",
       " 'h',\n",
       " 'app',\n",
       " 'iness',\n",
       " '>',\n",
       " '▁<',\n",
       " 'des',\n",
       " 'cribed',\n",
       " '▁by',\n",
       " '▁source',\n",
       " '>',\n",
       " '▁<',\n",
       " 'En',\n",
       " 'cyc',\n",
       " 'lop',\n",
       " '\\\\',\n",
       " 'u',\n",
       " '0',\n",
       " '0',\n",
       " 'E',\n",
       " '6',\n",
       " 'dia',\n",
       " '▁Britannica',\n",
       " '▁',\n",
       " '1',\n",
       " '1',\n",
       " 'th',\n",
       " '▁edition',\n",
       " '>',\n",
       " '▁.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(happiness_uniproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc1904fe-2b27-4f03-b54e-ef769c648c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/data/props.json') as fd:\n",
    "    obj = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daf16236-8f69-4e29-b032-27ce3cf3743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "obj = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eecf2d22-8d4f-4672-a9a5-ee84356e0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 {'datatype': 'external-id', 'id': 'P11322', 'label': '18th Century Russian Dictionary ID', 'description': 'identifier for a lexeme in the Словарь русского языка XVIII века (1984-1991) as hosted on the Fundamental Electronic Library of Russian Literature and Folklore', 'aliases': [], 'types': []}\n"
     ]
    }
   ],
   "source": [
    "for i,ob in enumerate(obj):\n",
    "    if 'Russian Literature' in ob['description']:\n",
    "        print(i, ob)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99fc95c6-26b4-44c2-99f6-15bcc92977b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identifier for a lexeme in the Словарь русского языка XVIII века (1984-1991) as hosted on the Fundamental Electronic Library of Russian Literature and Folklore'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[10]['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7b40e-93ed-40a7-91c6-f32ec65fafc6",
   "metadata": {},
   "source": [
    "## Verify titles conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3bda8245-efb0-4447-815e-fbc3a001fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/data/wikidata_titles_mapping.pickle', 'rb') as fd:\n",
    "    title_mappings = pickle.load(fd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e6c01fa-dd22-46c1-95ff-82228703f6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6199"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(title_mappings.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9bd7466f-b345-4213-911d-52f3b5d40eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ab70abc6-16f0-4587-a5f7-55760eedd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in title_mappings.items():\n",
    "    if v not in inverted_mapping:\n",
    "        inverted_mapping[v] = []\n",
    "    inverted_mapping[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "72bc53be-7d51-45ba-ad75-292d5012b130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5846104"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3f368c5-a061-4bc9-a38f-d40eb7f5d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('Anarchism', [6199])\n"
     ]
    }
   ],
   "source": [
    "for i,(k,v) in enumerate(inverted_mapping.items()):\n",
    "    if len(v) > 0:\n",
    "        print(i, (k,v))\n",
    "        break\n",
    "#found categories, portal, template. can I remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba8e44aa-ba52-43cb-97aa-1f3d7e8512b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [len(v) for v in inverted_mapping.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6da3705f-4772-49a8-bb5d-9506624fc555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b0fc755-b89d-492f-a65a-110b9cab9908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "14f75fb1-6dcd-4f37-a3dd-e241b9eaf604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(stats) / len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "68883645-caf0-457d-a6f6-f03e889ae083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5adfd2be-9226-4303-968b-11a52f7cba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c7dba36-b14d-4585-86f5-cda5196356b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.0), np.float64(1.0))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(stats, 0.97), np.quantile(stats, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "395d5777-9148-40f7-a6ad-3cdcc9e5cff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5846104, 0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats), sum(l for l in stats if l > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "82e5f739-8467-4716-ab3a-b36276aab3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous = {k:v for k,v in inverted_mapping.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22891768-b5c3-42c1-9d4e-7e25874e1fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74110d-9b53-4186-a909-151ea983cf78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "condabase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
