{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371",
   "metadata": {
    "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371"
   },
   "source": [
    "## Tutorial: Optimizing a Prompt\n",
    "\n",
    "![TextGrad](https://github.com/vinid/data/blob/master/logo_full.png?raw=true)\n",
    "\n",
    "An autograd engine -- for textual gradients!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Prompt-Optimization.ipynb)\n",
    "[![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n",
    "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n",
    "[![Documentation Status](https://readthedocs.org/projects/textgrad/badge/?version=latest)](https://textgrad.readthedocs.io/en/latest/?badge=latest)\n",
    "[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/textgrad)](https://pypi.org/project/textgrad/)\n",
    "[![PyPI](https://img.shields.io/pypi/v/textgrad)](https://pypi.org/project/textgrad/)\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* In this tutorial, we will run prompt optimization.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* You need to have an OpenAI API key to run this tutorial. This should be set as an environment variable as OPENAI_API_KEY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7add4547-4278-411b-a827-79be521851f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:30:34.029594610Z",
     "start_time": "2024-06-11T19:30:34.024175489Z"
    },
    "id": "7add4547-4278-411b-a827-79be521851f1",
    "outputId": "b5e1d95c-ebca-4633-db1e-4b6da8206423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install textgrad # you might need to restart the notebook after installing textgrad\n",
    "\n",
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e",
   "metadata": {
    "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e"
   },
   "source": [
    "Let's first define some support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccc3b21bf9ddc48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:30:42.098338405Z",
     "start_time": "2024-06-11T19:30:42.093473103Z"
    },
    "id": "1ccc3b21bf9ddc48"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "649e06aef34d0990",
   "metadata": {
    "id": "649e06aef34d0990"
   },
   "outputs": [],
   "source": [
    "def eval_sample(item, eval_fn, model):\n",
    "    \"\"\"\n",
    "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
    "\n",
    "    \"\"\"\n",
    "    x, y = item\n",
    "\n",
    "    x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
    "    y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
    "    response = model(x)\n",
    "    #try:\n",
    "    eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
    "    return int(eval_output_variable.value)\n",
    "    #except:\n",
    "    #    eval_output_variable = eval_fn([x, y, response])\n",
    "    #    eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n",
    "    #    return int(eval_output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9559a31e07e54d7f",
   "metadata": {
    "id": "9559a31e07e54d7f"
   },
   "outputs": [],
   "source": [
    "def eval_dataset(test_set, eval_fn, model, max_samples: int=None):\n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_set)\n",
    "    accuracy_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = []\n",
    "        for _, sample in enumerate(test_set):\n",
    "\n",
    "            future = executor.submit(eval_sample, sample, eval_fn, model)\n",
    "            futures.append(future)\n",
    "            if len(futures) >= max_samples:\n",
    "                break\n",
    "        tqdm_loader = tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0)\n",
    "        for future in tqdm_loader:\n",
    "            acc_item = future.result()\n",
    "            accuracy_list.append(acc_item)\n",
    "            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
    "    return accuracy_list\n",
    "\n",
    "def eval_dataset_slow(test_set, eval_fn, model, max_samples: int=None):\n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_set)\n",
    "    accuracy_list = []\n",
    "    tqdm_loader = tqdm(test_set)\n",
    "    for sample in tqdm_loader:\n",
    "        acc_item = eval_sample(sample, eval_fn, model)\n",
    "        accuracy_list.append(acc_item)\n",
    "        tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea732b7edf34eb9",
   "metadata": {
    "id": "4ea732b7edf34eb9"
   },
   "outputs": [],
   "source": [
    "def run_validation_revert(system_prompt: tg.Variable, results, model, eval_fn, val_set):\n",
    "    val_performance = np.mean(eval_dataset(val_set, eval_fn, model))\n",
    "    previous_performance = np.mean(results[\"validation_acc\"][-1])\n",
    "    print(\"val_performance: \", val_performance)\n",
    "    print(\"previous_performance: \", previous_performance)\n",
    "    previous_prompt = results[\"prompt\"][-1]\n",
    "\n",
    "    if val_performance < previous_performance:\n",
    "        print(f\"rejected prompt: {system_prompt.value}\")\n",
    "        system_prompt.set_value(previous_prompt)\n",
    "        val_performance = previous_performance\n",
    "\n",
    "    results[\"validation_acc\"].append(val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccaae3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom engine\n",
    "# I need to directly use huggingface locally for constrained generation\n",
    "\n",
    "from textgrad.engine.base import EngineLM, CachedEngine\n",
    "import platformdirs\n",
    "import os\n",
    "import importlib\n",
    "from transformers import LogitsProcessorList\n",
    "from ctrie import DictIndex, ConstrainedStateList, ConstrainedState, ConstrainedLogitsProcessor\n",
    "import torch\n",
    "\n",
    "class ChatConstrainedHF(EngineLM, CachedEngine):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config_path: str,\n",
    "        index_config_path: str,\n",
    "        system_prompt: None,\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        :param model_string:\n",
    "        :param system_prompt:\n",
    "        \"\"\"\n",
    "\n",
    "        if model_config_path.endswith('.py'):\n",
    "            model_config_path = model_config_path[:-3]\n",
    "        model_module = importlib.import_module(model_config_path)\n",
    "        self.model_config = getattr(model_module, 'model_config')\n",
    "        self.model_config.model.eval()\n",
    "\n",
    "        if system_prompt is None:\n",
    "            system_prompt = self.model_config.apply_prompt_template()\n",
    "\n",
    "        if index_config_path.endswith('.py'):\n",
    "            index_config_path = index_config_path[:-3]\n",
    "        index_module = importlib.import_module(index_config_path)\n",
    "        index_config = getattr(index_module, 'index_config')\n",
    "\n",
    "        assert index_config.rootkey > max(self.model_config.tokenizer.vocab.values())\n",
    "\n",
    "        root = platformdirs.user_cache_dir(\"textgrad\")\n",
    "        cache_path = os.path.join(root, f\"cache_hf_{model_config_path}.db\")\n",
    "\n",
    "        super().__init__(cache_path=cache_path)\n",
    "\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "        self.model = self.model_config.model\n",
    "\n",
    "        self.is_multimodal = False\n",
    "\n",
    "        self.num_states = self.model_config.generate_args.get('num_beams', 1)\n",
    "        self.states = ConstrainedStateList(\n",
    "            [ConstrainedState(\n",
    "                    begin_pattern = self.model_config.switch_pattern,\n",
    "                    end_pattern = self.model_config.newline_token,\n",
    "                    cache_index = DictIndex(end_of_triple=index_config.end_of_triple),\n",
    "                    subtree_cache = DictIndex(end_of_triple=index_config.end_of_triple),\n",
    "                    oneleaf_cache = DictIndex(end_of_triple=index_config.end_of_triple),\n",
    "                ) for _ in range(self.num_states)],\n",
    "            num_beams=self.model_config.generate_args.get('num_beams', 1),\n",
    "            batch_size = self.model_config.batch_size,\n",
    "            pad_token_id = self.model_config.generate_args['pad_token_id'])\n",
    "\n",
    "        self.constrained_processor = ConstrainedLogitsProcessor(\n",
    "            index=index_config.index,\n",
    "            end_token=self.model_config.newline_token,\n",
    "            states=self.states,\n",
    "            tokenizer=self.model_config.tokenizer\n",
    "            )\n",
    "        self.logits_processor_list = LogitsProcessorList([\n",
    "            self.constrained_processor\n",
    "        ])\n",
    "\n",
    "    def generate(self, prompt: str, system_prompt: str=None):\n",
    "        sys_prompt_arg = system_prompt if system_prompt else self.system_prompt\n",
    "        '''\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_arg},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        '''\n",
    "        self.model_config.model.eval()\n",
    "        with torch.no_grad():\n",
    "            #batch = [messages]\n",
    "            #prompted_batch = list(map(self.model_config.apply_prompt_template, batch))\n",
    "            prompted_batch = [system_prompt + prompt]\n",
    "\n",
    "            self.states.reset() # reset caches\n",
    "\n",
    "            batch_inputs = self.model_config.tokenize_fun(prompted_batch)\n",
    "\n",
    "            if len(self.states) != batch_inputs.input_ids.shape[0] * self.model_config.generate_args.get('num_beams', 1):\n",
    "                self.states = self.states[:batch_inputs.input_ids.shape[0] * self.model_config.generate_args.get('num_beams', 1)]\n",
    "                self.states.batch_size = batch_inputs.input_ids.shape[0]\n",
    "                self.constrained_processor.states = self.states\n",
    "\n",
    "            output = self.model_config.model.generate(\n",
    "                **batch_inputs,\n",
    "                #logits_processor=self.logits_processor_list,\n",
    "                **self.model_config.generate_args,\n",
    "                kwargs = {'constrained_state': self.states}, # passing stat\n",
    "            )\n",
    "\n",
    "            self.states.beam_permutation() # final permutation to match final beams\n",
    "\n",
    "            full_prediction = self.model_config.tokenizer.decode(output[0][len(batch_inputs.input_ids[0]):])\n",
    "            prediction = self.model_config.get_prediction(full_prediction)\n",
    "            prediction_complete = bool(prediction)\n",
    "\n",
    "            response = full_prediction\n",
    "            self._save_cache(sys_prompt_arg + prompt, response)\n",
    "\n",
    "            return response\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        return self.generate(prompt, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "103dfb07-6c3f-4b64-8ffa-8195c30dddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_config_path):\n",
    "    if dataset_config_path.endswith('.py'):\n",
    "        dataset_config_path = dataset_config_path[:-3]\n",
    "    dataset_module = importlib.import_module(dataset_config_path)\n",
    "    dataset = getattr(dataset_module, 'dataset')\n",
    "    dataset_x_y = [(item['question'], item['answer']['mention']) for item in dataset]\n",
    "    return dataset_x_y\n",
    "\n",
    "train_path = 'mintaka_train_ssample200'\n",
    "train_set = load_dataset(train_path)\n",
    "val_path = 'mintaka_dev_ssample72'\n",
    "val_set = load_dataset(val_path)\n",
    "test_path = 'mintaka_test_ssample200'\n",
    "test_set = load_dataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb78651-2a5f-4d86-8c4c-6a9409c566fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm_api_test\n",
    "llm_api_test = ChatConstrainedHF(\n",
    "    model_config_path=\"qwen25_1B_model\",\n",
    "    index_config_path=\"qwen25_index\",\n",
    "    system_prompt=None,\n",
    "    cache_path=os.path.join(platformdirs.user_cache_dir(\"textgrad\"), \"cache_hf_llama1.db\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2453cd0d-7ae7-41b0-901e-aeee7c49fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrad.autograd.string_based_ops import StringBasedFunction\n",
    "def answer_equality_fn(prediction: tg.Variable, ground_truth_answer: tg.Variable):\n",
    "    # TODO can consider other stuff like answer in prediction or viceversa\n",
    "    # can consider if there are triples generated\n",
    "    return int(str(prediction.value) == str(ground_truth_answer.value))\n",
    "\n",
    "fn_purpose = \"The runtime of string-based function that checks if the prediction is correct.\"\n",
    "eval_fn = StringBasedFunction(answer_equality_fn, function_purpose=fn_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "607a502d-8aa9-4f2c-9220-5f88518acaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 14:50:16 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from textgrad.engine.vllm import ChatVLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
   "metadata": {
    "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
    "outputId": "88a5a39d-b34c-4a7f-e17d-5e608eecad29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 14:50:18 [config.py:2704] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-08 14:50:27 [config.py:600] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 04-08 14:50:27 [arg_utils.py:1708] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 04-08 14:50:27 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-08 14:50:29 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-08 14:50:29 [cuda.py:289] Using XFormers backend.\n",
      "INFO 04-08 14:50:30 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-08 14:50:30 [model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 04-08 14:50:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-08 14:50:31 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870da400f1d94b05b2afbac7724253c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 14:50:32 [loader.py:447] Loading weights took 1.27 seconds\n",
      "INFO 04-08 14:50:32 [model_runner.py:1146] Model loading took 2.8876 GiB and 2.089426 seconds\n",
      "INFO 04-08 14:50:37 [worker.py:267] Memory profiling takes 4.36 seconds\n",
      "INFO 04-08 14:50:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.57GiB) x gpu_memory_utilization (0.90) = 13.11GiB\n",
      "INFO 04-08 14:50:37 [worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 04-08 14:50:37 [executor_base.py:112] # cuda blocks: 19081, # CPU blocks: 9362\n",
      "INFO 04-08 14:50:37 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 9.32x\n",
      "INFO 04-08 14:50:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 14:51:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.19 GiB\n",
      "INFO 04-08 14:51:04 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 32.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(12)\n",
    "llm_api_eval = ChatVLLM(model_string='Qwen/Qwen2.5-1.5B-Instruct', dtype='half') # torch.bfloat16\n",
    "tg.set_backward_engine(llm_api_eval, override=True)\n",
    "\n",
    "STARTING_SYSTEM_PROMPT = llm_api_test.system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676",
   "metadata": {
    "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676"
   },
   "source": [
    "This is the system prompt we are going to start from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950",
   "metadata": {
    "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful question-answering assistant that bases its answers on facts from a knowledge base and always respects the prompt.\n",
      "\n",
      "Process:\n",
      "\n",
      "    You receive an input question.\n",
      "\n",
      "    You get some context about the entities in the questions using description and short description.\n",
      "\n",
      "    You determine the reasoning path needed to answer the question based on the information available.\n",
      "\n",
      "    You explicitly list relevant facts, one per line, starting with \"Fact:\".\n",
      "\n",
      "    You explain your reasoning step by step and provide a detailed answer, justifying it based on the facts.\n",
      "\n",
      "    You conclude with a short and concise answer that must be grounded by the facts you found.\n",
      "\n",
      "You have to answer only basing on the facts you find in the knowledge base.\n",
      "If you cannot find relevant facts in the knowledge base to support an answer you stop and you reply: \"I don't know.\".\n",
      "After generating facts, please reason on top of the facts you find and avoid generating lot of useless facts.\n",
      "\n",
      "You must always follow these instructions precisely and ensure your responses adhere strictly to this prompt.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which mountain is taller between Mont Blanc and Mount Rainier?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: I need to provide the height of Mont Blanc and the height of Mount Rainier, then I need to compare the two heights and the final answer will be the taller mountain.\n",
      "Let's try to get a bit of context about these two mountains starting with the descriptions.\n",
      "Fact: <Mont Blanc> <description> <highest mountain in the Alps> .\n",
      "I found that Mont Blanc is the highest mountain in the Alps, but I didn't find its height.\n",
      "Fact: <Mount Rainier> <description> <stratovolcano in the U.S. state of Washington> .\n",
      "I found that Mount Rainier is a stratovolcano and its location. But I didn't find its height.\n",
      "Fact: <Mont Blanc> <short description> <Highest mountain in the Alps (4,808 m)> .\n",
      "I found the height of Mont Blanc.\n",
      "Fact: <Mount Rainier> <short description> <Stratovolcano in the U.S. state of Washington> .\n",
      "I found again that Mount Rainier is a stratovolcano, but I still need to find its height.\n",
      "Fact: <Mount Rainier> <elevation above sea level> <4,389 meters> .\n",
      "I found Mount Rainier elevation above sea level.\n",
      "Now that I know the height of both mountains I can give a grounded answer.\n",
      "Long answer: Mont Blanc is 4,808 meters tall, while Mount Rainier is 4,389 meters, so Mont Blanc is taller than Mount Rainier.\n",
      "\n",
      "Answer: Mont Blanc.<|im_end|>\n",
      "<|im_start|>user\n",
      "When was the director of Slumdog Millionaire born?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find the birth date of the director of Slumdog Millionaire. Let's start with Slumdog Millionaire description.\n",
      "Fact: <Slumdog Millionaire> <description> <2008 film directed by Danny Boyle> .\n",
      "I found that the director of Slumdog Millionaire is Danny Boyle. Now I need the birth date of Danny Boyle.\n",
      "Fact: <Danny Boyle> <date of birth> <1956-10-20T00:00:00Z>.\n",
      "Now I have all the information I need for answering.\n",
      "Long answer: The director of Slumdog Millionaire is Danny Boyle, who was born on October 20, 1956, so this is the answer to the question.\n",
      "\n",
      "Answer: October 20, 1956.<|im_end|>\n",
      "<|im_start|>user\n",
      "When did John V, Prince Of Anhalt-Zerbst's father die?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to find who the father of John V, Prince of Anhalt-Zerbst, was and then find the date of his death. Let's start with the description.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <description> <Prince of Anhalt-Dessau and Anhalt-Zerbst> .\n",
      "Ok, I still need to find his father.\n",
      "Fact: <John V, Prince of Anhalt-Zerbst> <father> <Ernest I, Prince of Anhalt-Dessau> .\n",
      "Now I know who is the father. I need to find when he died.\n",
      "Fact: <Ernest I, Prince of Anhalt-Dessau> <date of death> <1516-06-12T00:00:00Z> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: The father of John V, Prince of Anhalt-Zerbst, Ernest I, Prince of Anhalt-Dessau, died on June 12, 1516.\n",
      "\n",
      "Answer: June 12, 1516.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is Johnny Depp older than Brad Pitt?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine if Johnny Depp is older than Brad Pitt, I need to find their respective birth years. Let's start with the description.\n",
      "Fact: <Johnny Depp> <description> <American actor (born 1963)> .\n",
      "I found the year of birth of Johnny Depp. I need Brad Pitt's one.\n",
      "Fact: <Brad Pitt> <description> <American actor and filmmaker> .\n",
      "I cannot find Brad Pitt date of birth in the description.\n",
      "Fact: <Brad Pitt> <short description> <American actor (born 1963)> .\n",
      "I found Brad Pitt's year of birth. But since it is the same as Johnny Depp's, I need more precise information about their date of birth.\n",
      "Fact: <Johnny Depp> <date of birth> <1963-06-09T00:00:00Z> .\n",
      "I found when Johnny Depp was born, now I need Brad Pitt's date of birth.\n",
      "Fact: <Brad Pitt> <date of birth> <1963-12-18T00:00:00Z> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Johnny Depp was born on June 9, 1963, while Brad Pitt was born on December 18, 1963. So yes, Johnny Depp is older than Brad Pitt.\n",
      "\n",
      "Answer: Yes.<|im_end|>\n",
      "<|im_start|>user\n",
      "During which years did Napoleon hold power in France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To determine the years Napoleon held power in France, I need to identify the periods during which he held official leadership roles such as First Consul or Emperor. Maybe I can get this information from the description.\n",
      "Fact: <Napoleon Bonaparte> <description> <French military leader, French Emperor 1804–1814 and again in 1815> .\n",
      "Now I have all the information I need to give a grounded answer.\n",
      "Long answer: Based on the description, Napoleon Bonaparte held power in France as Emperor from 1804 to 1814, and briefly again in 1815.\n",
      "\n",
      "Answer: From 1799 to 1814, and in 1815.<|im_end|>\n",
      "<|im_start|>user\n",
      "In which books of A Song of Ice and Fire is Jon Snow not the Lord Commander of the Night's Watch?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: To answer this question, I need to consider the books in the A Song of Ice and Fire series by George R.R. Martin and identify the ones where Jon Snow is not the Lord Commander of the Night's Watch. Let's start with the description.\n",
      "Fact: <A Game of Thrones> <description> <1996 novel by George R. R. Martin> .\n",
      "This fact doesn't give me any information about Jon Snow.\n",
      "Fact: <A Game of Thrones> <publication date> <1996-08-06T00:00:00Z> .\n",
      "This fact is not giving any information about Jon Snow. I think the publication date won't help me.\n",
      "Fact: <A Clash of Kings> <publication date> <1998-11-16T00:00:00Z> .\n",
      "These facts don't contain any information about Jon Snow. I'll try one more time.\n",
      "Fact: <A Storm of Swords> <publication date> <2000-08-08T00:00:00Z> .\n",
      "I'm not able to find the facts about the books in which Jon Snow is not the Lord Commander, so I cannot give a grounded answer.\n",
      "Long answer: I don't know. I was not able to find the relevant facts in the knowledge base to motivate an answer.\n",
      "\n",
      "Answer: I don't know.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(STARTING_SYSTEM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d941a09-c34c-49de-8a46-983215284374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "test_set = test_set[:4]\n",
    "val_set = val_set[:4]\n",
    "train_set = train_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7544127-38e0-4c74-8632-003efcc645ee",
   "metadata": {
    "id": "f7544127-38e0-4c74-8632-003efcc645ee",
    "outputId": "879c12c1-b24d-47d7-aa8d-90906df4acbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0: 100%|██████████| 4/4 [01:27<00:00, 21.96s/it]\n",
      "Accuracy: 0.0: 100%|██████████| 4/4 [03:27<00:00, 51.91s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = tg.tasks.DataLoader(train_set, batch_size=3, shuffle=True)\n",
    "\n",
    "\n",
    "# Testing the 0-shot performance of the evaluation engine\n",
    "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"system prompt to the language model\")\n",
    "model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt)\n",
    "\n",
    "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
    "model = tg.BlackboxLLM(llm_api_test, system_prompt)\n",
    "\n",
    "optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[system_prompt])\n",
    "\n",
    "results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n",
    "results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
    "results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
    "results[\"prompt\"].append(system_prompt.get_value())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3807736-1d81-4349-95db-257c20110d1a",
   "metadata": {
    "id": "b3807736-1d81-4349-95db-257c20110d1a",
    "outputId": "b75a8864-bee3-4dbe-f2d2-b3002765fddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0. Epoch 0: : 0it [00:00, ?it/s]\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.02s/it, est. speed input: 142.89 toks/s, output: 34.35 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:48<00:00, 48.30s/it, est. speed input: 54.51 toks/s, output: 41.41 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 377.96 toks/s, output: 42.36 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.98s/it, est. speed input: 59.49 toks/s, output: 46.53 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it, est. speed input: 198.49 toks/s, output: 34.39 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it, est. speed input: 1263.05 toks/s, output: 50.52 toks/s]\n",
      "\n",
      "\u001b[Acessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.83s/it, est. speed input: 1535.64 toks/s, output: 3.95 toks/s]\n",
      "Training step 0. Epoch 0: : 0it [02:36, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "TextualGradientDescent optimizer response could not be indexed. This can happen if the optimizer model cannot follow the instructions. You can try using a stronger model, or somehow reducing the context of the optimization. Response: user\nIn which books of A Song of Ice and Fire is Jon Snow not the Lord Commander of the Night's Watch?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py:181\u001b[0m, in \u001b[0;36mTextualGradientDescent.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mnew_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_variable_tags\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_variable_tags[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Check if we got a cannot be indexed error\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39msum(losses)\n\u001b[1;32m     16\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m run_validation_revert(system_prompt, results, model, eval_fn, val_set)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msys prompt: \u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/textgrad/optimizer/optimizer.py:185\u001b[0m, in \u001b[0;36mTextualGradientDescent.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextualGradientDescent optimizer response could not be indexed\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer.response\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_text})\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextualGradientDescent optimizer response could not be indexed. This can happen if the optimizer model cannot follow the instructions. You can try using a stronger model, or somehow reducing the context of the optimization. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m parameter\u001b[38;5;241m.\u001b[39mset_value(new_value)\n\u001b[1;32m    187\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextualGradientDescent updated text\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter.value\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameter\u001b[38;5;241m.\u001b[39mvalue})\n",
      "\u001b[0;31mIndexError\u001b[0m: TextualGradientDescent optimizer response could not be indexed. This can happen if the optimizer model cannot follow the instructions. You can try using a stronger model, or somehow reducing the context of the optimization. Response: user\nIn which books of A Song of Ice and Fire is Jon Snow not the Lord Commander of the Night's Watch?"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    for steps, (batch_x, batch_y) in enumerate((pbar := tqdm(train_loader, position=0))):\n",
    "        pbar.set_description(f\"Training step {steps}. Epoch {epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for (x, y) in zip(batch_x, batch_y):\n",
    "            x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
    "            y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
    "            response = model(x)\n",
    "            try:\n",
    "                eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
    "            except:\n",
    "                eval_output_variable = eval_fn([x, y, response])\n",
    "            losses.append(eval_output_variable)\n",
    "        total_loss = tg.sum(losses)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        run_validation_revert(system_prompt, results, model, eval_fn, val_set)\n",
    "\n",
    "        print(\"sys prompt: \", system_prompt)\n",
    "        test_acc = eval_dataset(test_set, eval_fn, model)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        results[\"prompt\"].append(system_prompt.get_value())\n",
    "        if steps == 3:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
