{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371",
      "metadata": {
        "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371"
      },
      "source": [
        "## Tutorial: Optimizing a Prompt\n",
        "\n",
        "![TextGrad](https://github.com/vinid/data/blob/master/logo_full.png?raw=true)\n",
        "\n",
        "An autograd engine -- for textual gradients!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Prompt-Optimization.ipynb)\n",
        "[![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n",
        "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n",
        "[![Documentation Status](https://readthedocs.org/projects/textgrad/badge/?version=latest)](https://textgrad.readthedocs.io/en/latest/?badge=latest)\n",
        "[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/textgrad)](https://pypi.org/project/textgrad/)\n",
        "[![PyPI](https://img.shields.io/pypi/v/textgrad)](https://pypi.org/project/textgrad/)\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "* In this tutorial, we will run prompt optimization.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "* You need to have an OpenAI API key to run this tutorial. This should be set as an environment variable as OPENAI_API_KEY.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7add4547-4278-411b-a827-79be521851f1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T19:30:34.029594610Z",
          "start_time": "2024-06-11T19:30:34.024175489Z"
        },
        "id": "7add4547-4278-411b-a827-79be521851f1",
        "outputId": "b5e1d95c-ebca-4633-db1e-4b6da8206423"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install textgrad # you might need to restart the notebook after installing textgrad\n",
        "\n",
        "import argparse\n",
        "import concurrent\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import textgrad as tg\n",
        "from textgrad.tasks import load_task\n",
        "import numpy as np\n",
        "import random\n",
        "load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e",
      "metadata": {
        "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e"
      },
      "source": [
        "Let's first define some support functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccc3b21bf9ddc48",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T19:30:42.098338405Z",
          "start_time": "2024-06-11T19:30:42.093473103Z"
        },
        "id": "1ccc3b21bf9ddc48"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649e06aef34d0990",
      "metadata": {
        "id": "649e06aef34d0990"
      },
      "outputs": [],
      "source": [
        "def eval_sample(item, eval_fn, model):\n",
        "    \"\"\"\n",
        "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
        "\n",
        "    \"\"\"\n",
        "    x, y = item\n",
        "    x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
        "    y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
        "    response = model(x)\n",
        "    try:\n",
        "        eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
        "        return int(eval_output_variable.value)\n",
        "    except:\n",
        "        eval_output_variable = eval_fn([x, y, response])\n",
        "        eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n",
        "        return int(eval_output_parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9559a31e07e54d7f",
      "metadata": {
        "id": "9559a31e07e54d7f"
      },
      "outputs": [],
      "source": [
        "def eval_dataset(test_set, eval_fn, model, max_samples: int=None):\n",
        "    if max_samples is None:\n",
        "        max_samples = len(test_set)\n",
        "    accuracy_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        futures = []\n",
        "        for _, sample in enumerate(test_set):\n",
        "\n",
        "            future = executor.submit(eval_sample, sample, eval_fn, model)\n",
        "            futures.append(future)\n",
        "            if len(futures) >= max_samples:\n",
        "                break\n",
        "        tqdm_loader = tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0)\n",
        "        for future in tqdm_loader:\n",
        "            acc_item = future.result()\n",
        "            accuracy_list.append(acc_item)\n",
        "            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
        "    return accuracy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea732b7edf34eb9",
      "metadata": {
        "id": "4ea732b7edf34eb9"
      },
      "outputs": [],
      "source": [
        "def run_validation_revert(system_prompt: tg.Variable, results, model, eval_fn, val_set):\n",
        "    val_performance = np.mean(eval_dataset(val_set, eval_fn, model))\n",
        "    previous_performance = np.mean(results[\"validation_acc\"][-1])\n",
        "    print(\"val_performance: \", val_performance)\n",
        "    print(\"previous_performance: \", previous_performance)\n",
        "    previous_prompt = results[\"prompt\"][-1]\n",
        "\n",
        "    if val_performance < previous_performance:\n",
        "        print(f\"rejected prompt: {system_prompt.value}\")\n",
        "        system_prompt.set_value(previous_prompt)\n",
        "        val_performance = previous_performance\n",
        "\n",
        "    results[\"validation_acc\"].append(val_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccaae3f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# custom engine\n",
        "# I need to directly use huggingface locally for constrained generation\n",
        "\n",
        "from textgrad.engine.base import EngineLM, CachedEngine\n",
        "import platformdirs\n",
        "import os\n",
        "import importlib\n",
        "from transformers import LogitsProcessorList\n",
        "from ctrie import DictIndex, ConstrainedStateList, ConstrainedState, ConstrainedLogitsProcessor\n",
        "import torch\n",
        "\n",
        "class ChatConstrainedHF(EngineLM, CachedEngine):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_config_path: str,\n",
        "        index_config_path: str,\n",
        "        system_prompt: None,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        :param model_string:\n",
        "        :param system_prompt:\n",
        "        \"\"\"\n",
        "\n",
        "        if model_config_path.endswith('.py'):\n",
        "            model_config_path = model_config_path[:-3]\n",
        "        model_module = importlib.import_module(model_config_path)\n",
        "        model_config = getattr(model_module, 'model_config')\n",
        "        model_config.model.eval()\n",
        "\n",
        "        if system_prompt is None:\n",
        "            system_prompt = model_config.apply_prompt_template()\n",
        "\n",
        "        if index_config_path.endswith('.py'):\n",
        "            index_config_path = index_config_path[:-3]\n",
        "        index_module = importlib.import_module(index_config_path)\n",
        "        index_config = getattr(index_module, 'index_config')\n",
        "\n",
        "        assert index_config.rootkey > max(model_config.tokenizer.vocab.values())\n",
        "\n",
        "        root = platformdirs.user_cache_dir(\"textgrad\")\n",
        "        cache_path = os.path.join(root, f\"cache_hf_{model_config_path}.db\")\n",
        "\n",
        "        super().__init__(cache_path=cache_path)\n",
        "\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "        self.model = model_config.model\n",
        "\n",
        "        self.is_multimodal = False\n",
        "\n",
        "        self.num_states = self.model_config.generate_args.get('num_beams', 1)\n",
        "        self.states = ConstrainedStateList(\n",
        "            [ConstrainedState(\n",
        "                    begin_pattern = model_config.switch_pattern,\n",
        "                    end_pattern = model_config.newline_token,\n",
        "                    cache_index = DictIndex(end_of_triple=index_config.end_of_triple),\n",
        "                    subtree_cache = DictIndex(end_of_triple=index_config.end_of_triple),\n",
        "                    oneleaf_cache = DictIndex(end_of_triple=index_config.end_of_triple),\n",
        "                ) for _ in range(self.num_states)],\n",
        "            num_beams=model_config.generate_args.get('num_beams', 1),\n",
        "            batch_size = model_config.batch_size,\n",
        "            pad_token_id = model_config.generate_args['pad_token_id'])\n",
        "\n",
        "        self.constrained_processor = ConstrainedLogitsProcessor(\n",
        "            index=index_config.index,\n",
        "            end_token=model_config.newline_token,\n",
        "            states=self.states,\n",
        "            tokenizer=model_config.tokenizer\n",
        "            )\n",
        "        self.logits_processor_list = LogitsProcessorList([\n",
        "            self.constrained_processor\n",
        "        ])\n",
        "\n",
        "    def hf_generate(self, prompt: str, system_prompt: str=None):\n",
        "        sys_prompt_arg = system_prompt if system_prompt else self.system_prompt\n",
        "        messages=[\n",
        "                {\"role\": \"system\", \"content\": sys_prompt_arg},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        self.model_config.model.eval()\n",
        "        with torch.no_grad():\n",
        "            batch = [messages]\n",
        "            prompted_batch = list(map(self.model_config.apply_prompt_template, batch))\n",
        "\n",
        "            self.states.reset() # reset caches\n",
        "\n",
        "            batch_inputs = self.model_config.tokenize_fun(prompted_batch)\n",
        "\n",
        "            if len(states) != batch_inputs.input_ids.shape[0] * self.model_config.generate_args.get('num_beams', 1):\n",
        "                states = states[:batch_inputs.input_ids.shape[0] * self.model_config.generate_args.get('num_beams', 1)]\n",
        "                states.batch_size = batch_inputs.input_ids.shape[0]\n",
        "                self.constrained_processor.states = states\n",
        "\n",
        "            output = self.model_config.model.generate(\n",
        "                **batch_inputs,\n",
        "                logits_processor=self.logits_processor_list,\n",
        "                **self.model_config.generate_args,\n",
        "                kwargs = {'constrained_state': states}, # passing stat\n",
        "            )\n",
        "\n",
        "            states.beam_permutation() # final permutation to match final beams\n",
        "\n",
        "            full_prediction = self.model_config.tokenizer.decode(output[0][len(batch_inputs.input_ids[0]):])\n",
        "            prediction = self.model_config.get_prediction(full_prediction)\n",
        "            prediction_complete = bool(prediction)\n",
        "\n",
        "            response = full_prediction\n",
        "            self._save_cache(sys_prompt_arg + prompt, response)\n",
        "\n",
        "            return response\n",
        "\n",
        "    def __call__(self, prompt, **kwargs):\n",
        "        return self.generate(prompt, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
      "metadata": {
        "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
        "outputId": "88a5a39d-b34c-4a7f-e17d-5e608eecad29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val/Test Set Lengths:  83 83 84\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(dataset_config_path):\n",
        "    if dataset_config_path.endswith('.py'):\n",
        "        dataset_config_path = dataset_config_path[:-3]\n",
        "    dataset_module = importlib.import_module(dataset_config_path)\n",
        "    dataset = getattr(dataset_module, 'dataset').questions_dataset()\n",
        "\n",
        "def eval_fn(inputs):\n",
        "    \"\"\"\n",
        "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
        "    \"\"\"\n",
        "    prediction = inputs['prediction']\n",
        "    answer = inputs['ground_truth_answer']\n",
        "\n",
        "    loss = sum(predictions == answer)\n",
        "\n",
        "train_path = 'mintaka_train_ssample200'\n",
        "train_set = load_dataset(train_path)\n",
        "val_path = 'mintaka_dev_ssample72'\n",
        "val_set = load_dataset(val_path)\n",
        "test_path = 'mintaka_test_ssample200'\n",
        "test_set = load_dataset(test_path)\n",
        "eval_fn =\n",
        "\n",
        "loss = eval_fn(inputs=dict(prediction=prediction, ground_truth_answer=answer))\n",
        "\n",
        "\n",
        "\n",
        "set_seed(12)\n",
        "llm_api_eval = tg.get_engine(engine_name=\"gpt-4o\") # todo replace with big model running in vllm, ollama or whatever\n",
        "llm_api_test = ChatConstrainedHF(\n",
        "    model_config_path=\"qwen25_1B_model\",\n",
        "    index_config_path=\"http_index_qwen\",\n",
        "    system_prompt=None,\n",
        "    cache_path=os.path.join(platformdirs.user_cache_dir(\"textgrad\"), \"cache_hf_llama1.db\"),\n",
        ")\n",
        "tg.set_backward_engine(llm_api_eval, override=True)\n",
        "\n",
        "# Load the data and the evaluation function\n",
        "train_set, val_set, test_set, eval_fn = load_task(\"BBH_object_counting\", evaluation_api=llm_api_eval)\n",
        "print(\"Train/Val/Test Set Lengths: \", len(train_set), len(val_set), len(test_set))\n",
        "STARTING_SYSTEM_PROMPT = llm_api_test.system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676",
      "metadata": {
        "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676"
      },
      "source": [
        "This is the system prompt we are going to start from:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950",
      "metadata": {
        "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950"
      },
      "outputs": [],
      "source": [
        "print(STARTING_SYSTEM_PROMPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7544127-38e0-4c74-8632-003efcc645ee",
      "metadata": {
        "id": "f7544127-38e0-4c74-8632-003efcc645ee",
        "outputId": "879c12c1-b24d-47d7-aa8d-90906df4acbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7142857142857143: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1438.83it/s]\n",
            "Accuracy: 0.6867469879518072: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:00<00:00, 1318.42it/s]\n"
          ]
        }
      ],
      "source": [
        "train_loader = tg.tasks.DataLoader(train_set, batch_size=3, shuffle=True)\n",
        "\n",
        "\n",
        "# Testing the 0-shot performance of the evaluation engine\n",
        "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
        "                            requires_grad=True,\n",
        "                            role_description=\"system prompt to the language model\")\n",
        "model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt)\n",
        "\n",
        "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
        "                            requires_grad=True,\n",
        "                            role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
        "model = tg.BlackboxLLM(llm_api_test, system_prompt)\n",
        "\n",
        "optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[system_prompt])\n",
        "\n",
        "results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n",
        "results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
        "results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
        "results[\"prompt\"].append(system_prompt.get_value())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3807736-1d81-4349-95db-257c20110d1a",
      "metadata": {
        "id": "b3807736-1d81-4349-95db-257c20110d1a",
        "outputId": "b75a8864-bee3-4dbe-f2d2-b3002765fddf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5542168674698795: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:21<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.5542168674698795\n",
            "previous_performance:  0.6867469879518072\n",
            "rejected prompt: You will answer a reasoning question. Provide the final numerical answer directly. Your response should be a single numerical value. Do not include any explanation or additional text, only the numerical answer.\n",
            "sys prompt:  You will answer a reasoning question. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7142857142857143: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1201.24it/s]\n",
            "Accuracy: 0.8313253012048193: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:43<00:00,  1.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.8313253012048193\n",
            "previous_performance:  0.6867469879518072\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8214285714285714: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:43<00:00,  1.94it/s]\n",
            "Accuracy: 0.5542168674698795: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:46<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.5542168674698795\n",
            "previous_performance:  0.8313253012048193\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. \n",
            "\n",
            "1. Restate the question to ensure clarity.\n",
            "2. List each item you count on a new line using bullet points for clarity. If there are multiple items of the same type, list them together and indicate the quantity.\n",
            "3. Ensure the numbering and items in the list are consistent and free from typographical errors.\n",
            "4. After listing all items, count the total number of items and provide the final count in the specified format.\n",
            "5. Verify the total number by cross-checking with the list.\n",
            "6. If there are potential errors or ambiguities in the list, acknowledge them and request additional details if necessary.\n",
            "\n",
            "The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1638.96it/s]\n",
            "Accuracy: 0.6867469879518072: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:18<00:00,  4.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.6867469879518072\n",
            "previous_performance:  0.8313253012048193\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Directly compute the total number of items and provide the final count. Ensure you understand the context of the question and focus on counting the items accurately. Avoid adding any extra information or context that is not directly related to the total count. Ensure your response follows the format 'Answer: $VALUE' with no additional text. If the input contains unexpected characters or is malformed, correct the input and provide a coherent response. Ensure the final line contains only the answer in the required format.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1043.66it/s]\n",
            "Training step 3. Epoch 0: : 3it [07:29, 149.70s/it]\n",
            "Accuracy: 0.4819277108433735: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:34<00:00,  2.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.4819277108433735\n",
            "previous_performance:  0.8313253012048193\n",
            "rejected prompt: Answer a reasoning question that involves counting items in a list. Focus on counting only the items relevant to the question. Directly provide the total count without listing each item. Ensure the final line contains only the answer in the format 'Answer: $VALUE' where VALUE is a numerical value. Double-check your count for accuracy before providing the final number. If you encounter any ambiguous items or quantities, make a note of them and proceed with the calculation based on the clear items.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1660.49it/s]\n",
            "Accuracy: 0.3132530120481928: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:16<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.3132530120481928\n",
            "previous_performance:  0.8313253012048193\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Ensure you understand the context of the question, such as identifying specific categories of items (e.g., vegetables) before counting. Count the items and provide the total number. Be cautious of items that might be similar or easily confused, and ensure you are counting the correct items based on the context. If the query is ambiguous or could be interpreted in multiple ways, provide a brief explanation of your reasoning or ask for clarification. Ensure the response is a single numerical value without any additional text or formatting.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of the question and focus on counting the items accurately. List each item you count and then verify the total number. Avoid adding any extra information or context that is not directly related to the total count. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8214285714285714: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1674.21it/s]\n",
            "Accuracy: 0.891566265060241: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:00<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.891566265060241\n",
            "previous_performance:  0.8313253012048193\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [01:00<00:00,  1.38it/s]\n",
            "Accuracy: 0.8313253012048193: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:43<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.8313253012048193\n",
            "previous_performance:  0.891566265060241\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points with a hyphen (-) for each item. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, state the total number of items in the format: 'The total number of items listed is X.' Ensure the final line contains only the answer in the required format: 'Answer: $VALUE' where VALUE is a numerical value. Double-check your arithmetic to ensure the sum of the counts is correct.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1432.86it/s]\n",
            "Training step 3. Epoch 1: : 3it [08:16, 165.36s/it]\n",
            "Accuracy: 0.6144578313253012: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:18<00:00,  4.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.6144578313253012\n",
            "previous_performance:  0.891566265060241\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Provide only the final count of items without listing intermediate steps. Ensure you understand the context of each item and its relevance to the total count. If there is any ambiguity, ask a clarifying question. Present the numerical value clearly and directly, without any surrounding text or context. Ensure the final answer is a single numerical value without any additional text. Do not repeat the final answer; provide it only once in the specified format. Avoid using ellipses, parentheses, or any other punctuation that could complicate the response. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1608.54it/s]\n",
            "Accuracy: 0.5301204819277109: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:16<00:00,  5.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.5301204819277109\n",
            "previous_performance:  0.891566265060241\n",
            "rejected prompt: Count the items and provide the total number as a single integer. Do not include any additional text, explanations, or lists in your response. Ensure the answer is a numerical value only.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1517.28it/s]\n",
            "Accuracy: 0.8433734939759037: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:20<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.8433734939759037\n",
            "previous_performance:  0.891566265060241\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). Ensure that the numbering is sequential and does not skip any numbers. Avoid using ellipses or incomplete names; list each item explicitly. If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. Ensure that each item listed is distinct and not a duplicate. After listing the items, count them explicitly and verify that the total matches the number of distinct items listed. Explicitly show all mathematical operations in your reasoning. For example, if you are adding quantities, write out the full addition process. Before providing the final answer, double-check that the total count is accurate and matches the items listed. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1705.01it/s]\n",
            "Accuracy: 0.7710843373493976: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:27<00:00,  3.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_performance:  0.7710843373493976\n",
            "previous_performance:  0.891566265060241\n",
            "rejected prompt: You will answer a reasoning question that involves counting items in a list. Provide a concise answer without detailed explanations. List each item clearly and sequentially without using bullet points or numbering. Use consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, make a reasonable assumption and provide the answer directly. Ensure the final line contains only the answer in the following format: 'Answer: $VALUE' where VALUE is a numerical value.\n",
            "sys prompt:  You will answer a reasoning question that involves counting items in a list. Think step by step, but provide a concise summary of your reasoning. Ensure you understand the context of each item and its relevance to the total count. Use bullet points or numbering with periods for listing items. Maintain consistent naming conventions for similar items (e.g., Bed 1, Bed 2). If there is any ambiguity, provide reasoning for your choice or ask a clarifying question. After listing the items, verify the total count to ensure accuracy. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value. Ensure the final line contains only the answer in the required format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8571428571428571: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [00:00<00:00, 1062.32it/s]\n",
            "Training step 3. Epoch 2: : 3it [07:08, 142.76s/it]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(3):\n",
        "    for steps, (batch_x, batch_y) in enumerate((pbar := tqdm(train_loader, position=0))):\n",
        "        pbar.set_description(f\"Training step {steps}. Epoch {epoch}\")\n",
        "        optimizer.zero_grad()\n",
        "        losses = []\n",
        "        for (x, y) in zip(batch_x, batch_y):\n",
        "            x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
        "            y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
        "            response = model(x)\n",
        "            try:\n",
        "                eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
        "            except:\n",
        "                eval_output_variable = eval_fn([x, y, response])\n",
        "            losses.append(eval_output_variable)\n",
        "        total_loss = tg.sum(losses)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        run_validation_revert(system_prompt, results, model, eval_fn, val_set)\n",
        "\n",
        "        print(\"sys prompt: \", system_prompt)\n",
        "        test_acc = eval_dataset(test_set, eval_fn, model)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"prompt\"].append(system_prompt.get_value())\n",
        "        if steps == 3:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dab7a53-e682-478e-9417-15009b495979",
      "metadata": {
        "id": "7dab7a53-e682-478e-9417-15009b495979"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
